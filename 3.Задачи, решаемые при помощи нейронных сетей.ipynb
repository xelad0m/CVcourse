{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бинарная классификация? Бинарная кросс-энтропия!\n",
    "\n",
    "Мы коснулись задачи регрессии (аппроксимация скрытой зависимости), в которой нейронная сеть подбирает параметры активационных функций (этими параметрами являются веса и смещения скрытого слоя) и ищет оптимальную для аппроксимации линейную комбинацию этих активационных функций (веса выходного линейного слоя).\n",
    "\n",
    "**Задача бинарной классификации**\n",
    "\n",
    "Пусть у нас есть два класса. Мы хотим, чтобы сеть выдавала вероятность отнесения входа к одному из классов. Для этого вещественный выход линейного слоя дополнительно пропускается через сигмоиду и полученное значение интерпретируется как вероятность.\n",
    "\n",
    "\\*) Для задачи бинарной классификации можно обойтись и без выходной сигмоиды; но тогда сеть будет дольше обучаться, т.к. придется ориентироваться в пространстве решений бесконечного размера, а не в интервале [0...1].\n",
    "\n",
    "- выходная функция активации - сигмоида:\n",
    "\n",
    "$$\\sigma(y) = \\frac{1}{1 + e^{-y}} \\\\\n",
    "\\sigma'(y) =  \\sigma(y) (1 - \\sigma(y))$$\n",
    "\n",
    "- функция потерь:\n",
    "  - **почему не подходит MSE**? \n",
    "    - $MSE_i = (\\sigma(y_i) - t_i)^2$, где $t_i \\in \\{0,1\\}$ - это таргет (ground truth)\n",
    "    - $MSE_i' = 2 (\\sigma(y_i) - t_i) \\sigma' = 2 (\\sigma(y_i) - t_i) \\sigma(y_i)(1  - \\sigma(y_i))$\n",
    "      - потому что производная такой функции потерь \"слишком легко\" обращается в 0 (происходит паралич сети): три сомножителя, только один из которых зависит от таргета, остальные могут зануляться и обращать все в 0 (при малых значениях выходов $y_i$ будет $\\sigma \\rightarrow 0$, при больших значениях выходов $y_i$ будет $(1 - \\sigma) \\rightarrow 0$, а вместе $\\sigma (1 - \\sigma) \\leq 0.25$ при любых аргументах)\n",
    "\n",
    "**Функция потерь - бинарная кроссэнтропия**\n",
    "\n",
    "$BCE(p, t) = - t \\log p - (1 - t) \\log (1 - p)$, где $t \\in \\{0,1\\}, p=\\sigma(y) \\in (0, 1)$\n",
    "- например, $t=1, y\\rightarrow -\\infin$, тогда $\\log 0_+ \\rightarrow +\\infin, BCE \\rightarrow +\\infin$\n",
    "\n",
    "Производная бинарной кроссэнетропии\n",
    "\n",
    "$$\\frac {\\partial BCE}{\\partial y} = \\frac {\\partial BCE}{\\partial p}\\frac {\\partial p}{\\partial y} = \\frac{t}{\\sigma} \\sigma' + \\frac{1-t}{1-\\sigma} \\sigma' = -t (1 - \\sigma) + (1-t)\\sigma = \\sigma - t $$\n",
    "- помним, что $p=\\sigma(y)$\n",
    "- видим, что \"плохих\" сомножителей нет\n",
    "- бинарная кросс-энтропия - это функция потерь, которой хорошо оценивать вероятности и любые значения, которые находятся в интервале от 0 до 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Многоклассовая классификация? Софтмакс!\n",
    "\n",
    "В этом случае выход нейросети - это полносвязный линейный слой с числом выходов по количеству классов. По сути, это по-прежнему поиск оптимальной линейной комбинации функций активации, только теперь таких комбинаций ищется несколько, и они взаимосвязаны.\n",
    "\n",
    "**Выходная \"функция активации\"** - сфотмакс:\n",
    "$$SM_i(\\vec y) = \\frac {e^{y_i}}{\\sum_{j=1}^N e^{y_j}}$$\n",
    "- $0 < SM_i < 1, \\sum SM_i = 1$\n",
    "- softmax (по-русски это - **нормализованная экспоненциальная функция**) — это обобщение логистической функции для многомерного случая\n",
    "- экспонента тут нужна, чтобы нормировались также и отрицательные значения аргументов, наравне с положительными (отображение $\\R\\rightarrow (0,1)$), а также - потому что хорошие производные\n",
    "- не может обратиться в 0, не может обратиться в 1 (кроме вырожденного случая с 1 классом)\n",
    "- это на самом деле не функция активации в общепринятом смысле (преобразует выходы 1 нейрона), а преобразование пачки выходов (можно, конечно, выход тоже считать таким особо хитрым нейроном сети и тогда ОК), а вообще - это такой обоснованный мат.трюк \n",
    "\n",
    "Производная софтмакса ($(u/v)' = (u'v - v'u)/v^2$)\n",
    "- по выходам \"остальных классов\"\n",
    "\n",
    "$$\\frac {\\partial SM_i}{\\partial y_c} = \\frac{-e^{y_i}e^{y_c}}{\\left(\\sum_{j=1}^N e^{y_j} \\right) ^2} = -SM_i SM_c$$\n",
    "\n",
    "- по выходу данного класса\n",
    "\n",
    "$$\\frac {\\partial SM_i}{\\partial y_i} = \\frac{-e^{y_i} \\sum_{j=1}^N e^{y_j} - e^{y_i} e^{y_i}}{\\left(\\sum_{j=1}^N e^{y_j} \\right) ^2} = SM_i (1 - SM_i)$$\n",
    "\n",
    "**Функция потерь - кроссэнтропия**\n",
    "\n",
    "$$CE(p, t) = - \\sum_{c=1}^N t_c \\log p_c$$\n",
    "- где $t_c \\in \\{0,1\\}, p=SM_c \\in (0, 1)$\n",
    "\n",
    "Производная кроссэнтропии\n",
    "\n",
    "$$\\frac {\\partial CE}{\\partial y_i} = - \\frac{t_i}{p_i} \\frac {\\partial SM_i}{\\partial y_i} - \\sum_{c \\neq i} \\frac{t_c}{p_c} \\frac{\\partial SM_c}{\\partial y_i} = - \\frac{t_i}{p_i} SM_i (1 - SM_i) - \\sum_{c \\neq i} \\frac{t_c}{p_c} (-SM_c SM_i) \\Rightarrow \\\\\n",
    "\\frac {\\partial CE}{\\partial y_i} = - t_i (1 - p_i) + \\sum_{c \\neq i} t_c p_i \\Rightarrow \\\\ \n",
    "\\frac {\\partial CE}{\\partial y_i} = - t_i + p_i$$\n",
    "\n",
    "- производная по выходу сети - линейна\n",
    "- обращается в 0 только тогда, когда $p_i = t_i$\n",
    "- $p_i$ - уверенность, что на вход был подан класс $i$\n",
    "- $(1 - p_i)$ - суммарная уверенность во всех классах $c \\neq i$\n",
    "- просто красиво"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Локализация, детекция, сегментация и super-resolution\n",
    "\n",
    "\n",
    "### Задача детекции и локализации\n",
    "\n",
    "Обычно решаются совместно, но можно и по отдельности. Сочетает в себе решение задачи классификации и задачи восстановления скрытой зависимости (регрессии).\n",
    "\n",
    "- У нас есть картинка, и на этой картинке где-то есть объект (возможно)\n",
    "- Нам нужно на картинке найти, где этот объект находится\n",
    "\n",
    "1. **Детекция**. Начнём с того, что нам нужно построить какую-то сеть, которая будет принимать на вход картинку и будет выдавать следующее. Для начала нам нужно определиться с тем, есть или нет на этой картинке объект. Соответственно, один из выходов нейронной сети нам должен говорить вероятность: есть здесь объект, или нет, то есть, с какой вероятностью на этой картинке находится объект. Если объекта нет, то вероятность должна быть очень маленькая, а когда объект есть, вероятность должна стремиться к 1. \n",
    "\n",
    "2. **Локализация**. Кроме этого, мы можем по-разному параметризовать прямоугольник, который обозначает регион, где находится объект. Я предлагаю этот прямоугольник параметризовать следующим образом: во-первых, мы должны определить, где у этого объекта находится центр это вот этот вот жёлтый крестик. И также мы должны определить размеры объекта: его высоту и ширину.\n",
    "\n",
    "Соответственно, нейронная сеть должна нам выдавать 5 выходов:\n",
    "- $p$ - вероятность наличия объекта\n",
    "- $x_c, y_c$ - 2 координаты центра объекта на картинке\n",
    "- $w, h$ - ширина и высота объекта на картинке\n",
    "\n",
    "Введем **ограничения**:\n",
    "- на картинке должен быть один объект или ни одного\n",
    "  - чтобы не решать отдельную задачу, какой из объектов выбрать\n",
    "- если центр объекта лежит в пределах картинки, то считаем, что объект есть, иначе - объекта нет\n",
    "  - если видно только ухо кота, то считаем, что кота на картинке нет\n",
    "- объект может выходить за пределы картинки\n",
    "  - тогда выдаются ширина/высота, какие они должны быть, а не размер картинки\n",
    "\n",
    "\\*) Допустим, мы хотим локализовать (**распознать координаты**) объекта в кадре, используя **сигмоидную функцию активации**. Почему это не самый удачный вариант ФА? Эта функция очень насыщена ближе к 0 и к 1, в том смысле, что переход от y=0 к y=1 происходит в узком диапазоне, в большей части диапазона сигмоида равна либо 0 (слева), либо 1 (справа). Это приведёт к хорошему пространственному разрешению в области смены значения, но плохому разрешению по краям картинки... фактически здесь мы используем сигмоиду не по назначению, ожидая, что она будет вести себя как линейная функция. **Здесь (в задаче локализации) лучше просто использовать, например, линейную функцию активации, раз нам нужна проекция в непрерывное пространство координат**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выходные функции активации**\n",
    "\n",
    "В части детекции используется сигмодиа \n",
    "- $p = \\sigma(y_0)$ - интерпретируется как вероятность наличия объекта.\n",
    "\n",
    "В части локализации:\n",
    "- координаты центра\n",
    "  - сигмоиды - считаем нижний левый угол кадра (0,0), правый верхний (1,1) в масштабе этих ФА\n",
    "    - $x_c = \\sigma(y_1), y_c = \\sigma(y_2)$\n",
    "- ширина и высота\n",
    "  - можно тоже сигмоиды - но так не делают (высота/ширина снизу ограничены 0, а сверху по-идее они не ограничены)\n",
    "  - хороший вариант - **экспонента**:\n",
    "    -  $w = \\exp(y_3), h = \\exp(y_4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функция потерь**\n",
    "\n",
    "Выходы гетерогенные, функции потерь - тоже\n",
    "\n",
    "В части детекции используется (бинарная) кроссэнтропия \n",
    "- $p: BCE(\\sigma(y_0), \\tilde t)$ \n",
    "\n",
    "В части локализации:\n",
    "- координаты центра\n",
    "  - $x_c, y_c: BCE(\\sigma(y_1), \\tilde x_c), BCE(\\sigma(y_2), \\tilde y_c)$\n",
    "- ширина и высота\n",
    "  - можно средний квадрат ошибки, например, так: \n",
    "    - $w, h: MSE(y_3, \\log w), MSE(y_4, \\log h)$\n",
    "\n",
    "Итого:\n",
    "\n",
    "$$L = BCE(\\sigma(y_0), \\tilde t) + \\tilde t \\left(BCE(\\sigma(y_1), \\tilde x_c) + BCE(\\sigma(y_2), \\tilde y_c) + MSE(y_3, \\log w), MSE(y_4, \\log h)\\right)$$\n",
    "\n",
    "- где $\\tilde t$ также используется как индикатор для учета ошибки локализации объекта\n",
    "\n",
    "Если сеть предсказала ($p, xc, yc, w, h$), и $p > 0$, а на самом деле объекта на картинке нет, то\n",
    "- Значения функции потерь будут такими же, как если бы истинным значением было ($p=0, xc, yc, w, h$)\n",
    "- Мы штрафуем нейросеть только за ошибочную классификацию ($p$), не учитывая предсказанные координаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача сегментации\n",
    "\n",
    "Допустим, у нас есть картинка, у нас есть на ней объект, и мы хотим отделить те пиксели, на которых объект есть, от тех пикселей, на которых объекта нет.\n",
    "\n",
    "Мы берём кадр, пропускаем через сеть - что эта сеть должна нам выдавать? \n",
    "\n",
    "Предлагается следующий вариант: что эта сеть нам выдаёт кадр, которая состоит из \"скоров\" (score) того, что в каждом из пикселей есть или нет объект. Допустим, мы сделали такую сеть, которая выдаёт кадр, там есть \"скоры\" для каждого пикселя. Вопрос: как теперь сделать функцию потерь для этой задачи? **При этом для обучения у нас есть правильная сегментация объекта**.\n",
    "\n",
    "Получается **задача бинарной сегментации** - отделение объекта от фона\n",
    "\n",
    "**Выходная фукнция активации** \n",
    "- сигмоида - дает для каждого пикселя \"вероятность\" отнесения к объекту\n",
    "\n",
    "**Функция потерь**\n",
    "- суммарная бинарная кроссэнтропия по всем пискелям кадра\n",
    "\n",
    "$$L = \\sum_j BCE(\\sigma(y_j), t_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача сжатия размерности\n",
    "\n",
    "Есть кадр, в котором содержится большое количество разной информации. \n",
    "\n",
    "Мы хотим сделать сеть **(авто-)энкодер**, которая по кадру выдет **векторное представление информации**, содержащейся в кадре - **эмбеддинг**, и на котором другая сеть **декодер** делает некоторую расшифровку - переводит это векторное представление обратно в изображение.\n",
    "\n",
    "Внутренне векторное представление (эмбеддинг) будет меньшей размерности, чем исходный кадр, иначе смысла нет.\n",
    "\n",
    "**Функция потерь**\n",
    "- можно просто сравнивать входной и выходной кадр (например, попиксельный MSE)\n",
    "$$L = \\sum_i MSE(Img_i, \\tilde {Img_i}$$\n",
    "- можно перевести пиксели кадра в [0,1], на выход декодера - софтмакс, и использовать бинарную кроссэнтропию между входным и выходным кадром\n",
    "\n",
    "$$L = \\sum_i BCE(Img_i, \\sigma (y_i))$$\n",
    "\n",
    "В данном случае **целью обучения** сети является получение векторных представлений кадров (**получение эмбеддингов**), которые можно потом с хорошей эффективностью использовать в других задачах\n",
    "- например, сжимать кадр таким обученным автоэнкодером и на полученном эмбеддинге уже тренировать другую сеть решать задачу классификации объектов в кадре (или вообще без сети использовать - деревья решений, линейная регрессия и т.п.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача super-resolution\n",
    "\n",
    "Задача \"super resolution\" заключается в следующем: у нас есть какая-то картинка, с очень низким разрешением. Мы хотим это разрешение увеличить в 2 раза по каждому измерению, то есть по X и по Y. Как сделать такую нейронную сеть?\n",
    "\n",
    "Cеть принимает на вход картинку и на каждый пиксель входной картинки выдаёт 4 пикселя выходной картинки. \n",
    "\n",
    "Cеть можно обучать примерно так: исходный набор кадров высокого разрешения сжимается в 2 раза по каждому измерению, подается на вход и сравнивается выход и исходные кадры до сжатия.\n",
    "\n",
    "В отличие от алгоритмической пикселизации/интерполяции здесь нейросеть может дорисовать детали, которых на сжатом изображении нет, то которые были на обучающих кадрах (шерстинки кота, детали лица, текстуры одежды и т.п.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Другие задачи CV\n",
    "\n",
    "Стоит отметить, что класс задач, которые решаются при помощи нейронных сетей очень велик. И он далеко не ограничивается теми задачами, которые мы рассмотрели. Каждый год появляются новые задачи, нередко эти задачи попадают в класс \"классификации\" некоторые из этих задач попадают в разряд \"регрессии\". Но бывают такие, которые совмещают в себе и классификацию, и регрессию, и очень часто они бывают довольно хитрыми, и мы с вами уже разобрали, каким образом можно решать такие задачи: нужно придумывать функции активации и придумывать правильные функции потерь для того, чтобы решать эти задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические задачи: Функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть имеется монетка, которую мы подбрасывали $N$ раз, и $M$ раз монетка выпала орлом вверх. Мы будем восстанавливать вероятность выпадения **орла** $p$ при помощи минимизации бинарной кросс-энтропии:\n",
    "\n",
    "$$\\tilde{p} = \\arg\\min_p \\sum_{i=1}^N \\left( - t_i \\log p - (1 - t_i) \\log(1 - p) \\right),$$\n",
    "\n",
    "где $\\arg\\min_x f(x)$ - значение $x$, при котором $f$ минимальна, $t_i = 1$ в том случае, когда выпал орел и $t_i = 0$ в том случае, когда выпала решка.\n",
    "\n",
    "Найдите $\\tilde{p}$ и запишите это выражение в качестве ответа (синтаксис формул: python/sympy).\n",
    "\n",
    "$p = - M \\log p - (N-M) \\log(1 - p)$\n",
    "\n",
    "$p' = -M / p - (N-M) / (1 - p) = 0$\n",
    "\n",
    "$p_0 = M/N$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\operatorname{Set}\\left(\\mathtt{\\text{dL =}}, - \\frac{M}{p} - \\frac{M - N}{1 - p}, \\mathtt{\\text{  p0 =}}, \\left[ \\frac{M}{N}\\right]\\right)$"
      ],
      "text/plain": [
       "Set(dL =, -M/p - (M - N)/(1 - p),   p0 =, [M/N])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "p, N, M = sp.symbols(\"p N M\")\n",
    "\n",
    "L = - M * sp.log(p) - (N - M) * sp.log(1 - p)\n",
    "dL = L.diff(p)\n",
    "p0 = sp.solve(dL, p)\n",
    "\n",
    "sp.Set(\"dL =\", dL, \"  p0 =\", p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, имеется много наблюдений (при абсолютно одинаковых параметрах, - вообще не при чем тут): $\\{ y_i \\}_{i=1}^N$. Сформулируем задачу восстановления значения $\\mu$ следующим образом:\n",
    "\n",
    "$\\mu = \\arg\\min_{\\tilde{y}} \\sum_{i=1}^N(\\tilde{y} - y_i)^2$\n",
    "\n",
    "Найдите, чему будет равно $\\mu$ (т.е. $\\tilde{y}$ при котором выражение минимально):\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\tilde y} = 2\\sum_{i=1}^N(\\tilde{y} - y_i) = 2 N \\tilde{y} - 2 \\sum_{i=1}^N y_i = 0 \\\\\n",
    "\\tilde{y} = \\left(\\sum_{i=1}^N y_i \\right) / N $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, имеется много наблюдений при абсолютно одинаковых параметрах: $\\{ y_i \\}_{i=1}^{2N - 1}$. Сформулируем задачу восстановления значения $\\mu$ следующим образом:\n",
    "\n",
    "$\\mu = \\arg\\min_{\\tilde{y}} \\sum_{i=1}^{2N - 1} |\\tilde{y} - y_i|$\n",
    "\n",
    "Найдите, чему будет равно $\\tilde{y}$, минимизирующее функцию потерь, если все значения $y_i$ расположены по возрастанию. \n",
    "\n",
    "- можно нетривиально ковырять тривиальную производную, но тут все в уме - это такое значение, которое будет в середине упорядоченного ряда наблюдений, остальные в производной сокращаются\n",
    "\n",
    "$$\\tilde y = y_N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть имеется выборка $\\{ t_i \\}_{i=1}^N$ некоторых чисел $t_i \\in (0, 1)$\n",
    "\n",
    "Запишите решение оптимизационной задачи:\n",
    "\n",
    "$\\mu = \\arg\\min_{p} \\sum_{i=1}^N \\left(- t_i \\log p - (1 - t_i) \\log (1 - p)\\right)$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial p} = \\sum_{i=1}^N \\left(- \\frac{t_i}{p} - \\frac{1 - t_i} {1 - p} (-1)\\right) = - \\frac{1}{p}\\sum_{i=1}^N t_i + \\frac{1} {1 - p} \\sum_{i=1}^N (1 - t_i) = \\\\\n",
    "\n",
    "= - \\frac{1}{p}\\sum_{i=1}^N t_i + \\frac{N} {1 - p} - \\frac{1} {1 - p}\\sum_{i=1}^N t_i = \\frac{N} {1 - p} - \\frac{p + 1 - p}{p(1-p)}\\sum_{i=1}^N t_i = 0\\\\\n",
    "\n",
    "\\frac{N} {1 - p} = \\frac{\\sum_{i=1}^N t_i} {p(1 - p)} \\\\\n",
    "\n",
    "\n",
    "p = \\frac{\\sum_{i=1}^N t_i} {N} $$\n",
    "\n",
    "Опять метод максимального правдоподобия и матожидание как оценка параметра"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Семинары** по регрессии (восстановление неизвестной зависимости, аппроксимация) и классификации в `./Neural_Networks_and_CV`"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d00ba0b92f737b23b3e678e3a3ceb3fe4e948ad1ab95d9c6fdcbb4b4ec65f3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
