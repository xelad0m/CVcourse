{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ударим дропаутом по переобучению!\n",
    "\n",
    "## Проблема переобучения\n",
    "\n",
    "$$P_n(x) = \\sum_{k=0}^n \\alpha_k x^k$$\n",
    "$$L = \\sum_{i=0}^N (P_n(x) - y_i)^2$$\n",
    "\n",
    "$n$ - степерь полинома, $N$ - количество наблюдений\n",
    "\n",
    "\n",
    "Приближаем данные полиномом степени $n$ \n",
    "- n=0 - горизонтальная прямая (ср.арифм. или мат.ожидаение)\n",
    "- n=1 - линейная регрессия\n",
    "- ...\n",
    "- n=N - линия пройдет в точности через все точки наблюдения (если $L\\to min$, $L=0$) \n",
    "  - в этом случае говорят, что модель \"выучила\" обучающую выборку, но полученный полином совершенно не будет отражать фактическую зависимость в данных ($x$ от $y$)\n",
    "\n",
    "Когда у модели много параметров - она имеет тенденцию к переобучению, которого нужно избегать\n",
    "\n",
    "### Направления борьбы с переобучением\n",
    "\n",
    "1. Не усложнять модель больше чем нужно\n",
    "2. Увеличить количество данных\n",
    "3. Сравнение качества обучения с валидационной выборкой\n",
    "4. Регуляризация\n",
    "5. Дропаут\n",
    "\n",
    "Увеличение количества данных на основе имеющихся данных называется **аугментацией**\n",
    "- например, в случае изображений - создание измененных кадров на основе имеющихся\n",
    "  - геометрические преобразования (вырезания фаргментов, повороты, сжатия, отражения)\n",
    "  - изменения цвета, контрастности и т.п.\n",
    "  - добавление шумов\n",
    "    - шум \"соль-перец\" (случайно возникающие чёрные и белые пиксели в кадре)\n",
    "    - гауссовский шум\n",
    "  - разнообразные другие эффекты, сохраняющие исходный класс изображения\n",
    "- в валидационную выборку не должны попадать такие дополнительные данные\n",
    "\n",
    "Сравнение качества обучения с валидационной выборкой - \n",
    "- \"раннее завершение\" (early stopping) - остановка обучения, когда функция потерь на валидационной выборке начала расти\n",
    "  - как вариант - сохранять контрольные точки и по итогу выбирать состояние с минимальными потерями на валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация\n",
    "\n",
    "Изменение функции потерь с добавленим штрафа за слишком большие коэффициенты (веса сети, к-ты полинома)\n",
    "\n",
    "#### **Регуляризация Тихонова** ($L_2$-регуляризация / weight decay / ridge regression)\n",
    "\n",
    "$$\\tilde L = L + \\lambda \\sum_{k=0}^{\\#\\alpha} \\alpha_k^2$$\n",
    "\n",
    "- weight decay - это похожее, но не совсем это\n",
    "\n",
    "Проблема регуляризации - изменяется функция потерь и в результате оптимум ищется не совсем от нужного функционала. Есть значимая вероятность попасть в локальный минимум регуляризованной функции, который будет соответствовать пику исходной функции. \n",
    "\n",
    "**Пример**: $L(\\alpha) = cos(\\alpha) + \\lambda \\alpha^2$ - где периодический косинус представляет истинную функцию потерь, у которой много локальных минимумов, один из которых мы хотим найти. При \"больших\" $\\lambda$ парабола регулярзации \"продавит\" косинус так, что останутся только два варианта из всех возможных $\\{-\\pi, +\\pi\\}$. Если увеличить $\\lambda$ еще больше, то может получиться, что минимум окажется в 0, что вообще не соответствует реальному минимуму функции потерь.\n",
    "\n",
    "`AdamW` - это реализация `Adam` с применением **Weight Decay**. \n",
    "\n",
    "Что такое Weight Decay? При каждом обновлении веса давайте кроме движения в сторону антиградиента еще и будем вычитать маленький кусочек веса, умноженный на некоторый гиперпараметр. Например, формула стохастического градиентного спуска с применением Weight Decay будет выглядеть так:\n",
    "\n",
    "$w = w - lr * \\dfrac{\\partial L}{\\partial w} - lr * wd * w$ \n",
    "\n",
    "Здесь lr - learning rate, wd - weight decay, гиперпараметр.\n",
    "\n",
    "Если Вы внимательно посмотрите на эту формулу, то заметите, что в случае стохастического градиентного спуска Weight Decay эквивалентен применению L2-регуляризации к loss-функции:\n",
    "\n",
    "$$L_{new} = L + \\dfrac{wd}{2} ||w||^2 \\\\\n",
    " \n",
    "\\dfrac{\\partial (L_{new})}{\\partial w} = \\dfrac{\\partial L}{\\partial w} + wd \\cdot w \\\\\n",
    "\n",
    "w = w - lr \\cdot \\dfrac{\\partial L_{new}}{\\partial w} = w - lr * \\dfrac{\\partial L}{\\partial w} - lr * wd * w $$\n",
    "\n",
    "Однако, L2-регуляризация (**меняем loss-функцию**) и Weight Decay (**не меняем loss-функцию**, меняем только способ обновления весов) работают одинаково только в простом случае стохастического градиентного спуска, в случае адаптивных оптимизаторов, например, Адама, эти два подхода различаются (причем эмпирически было показано, что часто Weight Decay работает лучше). Вся эта история породила некоторую путаницу в терминилогии, и во многих библиотеках Адам реализовали именно с применением L2-регуляризации, ошибочно называя такой подход Weight Decay.  Авторы статьи [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) решили разграничить **Adam+L2** и **Adam+Weight Decay**, назвав последнее AdamW.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Регуляризация LASSO** ($L_1$-регуляризация)\n",
    "\n",
    "$$\\tilde L = L + \\lambda \\sum_{k=0}^{\\#\\alpha} | \\alpha_k|$$\n",
    "\n",
    "Линии уровня слагаемого регуляризации теперь не окружности как в $L_2$, а ромбы (размерности $\\#\\alpha$ конечно)\n",
    "- значит в сумме с исходной функцией потерь, минимум будет где-то на осях этих линий уровня, т.е. лассо-регуляризация \"прореживает\" параметры модели, зануляет менее значимые параметры\n",
    "- регуляризация Тихонова диффиренцируема в любой точке, LASSO - нет\n",
    "- по модулю производная LASSO всегда равна $\\lambda$, если она дифференцируема"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дропаут\n",
    "\n",
    "Вид регуляризации, характерный только для нейронных сетей.\n",
    "\n",
    "1 вариант (drop connection) - с вероятностью $p$ пропадает связь между нейронами ($w=0$)\n",
    "\n",
    "2 вариант (drop neuron) - с вероятностью $p$ нейрон перестает выдавать сигнал"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Батч-нормализация\n",
    "\n",
    "Нормализованные данные приводят к более быстрому обучению модели\n",
    "\n",
    "Если данные сильно смещены от начала координат, то потребуется много шагов, чтобы изначальная разделяющая поверхность, которая появилась в результате случайной инициализации, а значит где-то вокруг 0, дойдет до разделения удаленных в некотую сторону от 0 данных.\n",
    "\n",
    "Вытянутая вдоль какой-то оси форма данных также приводит вытягиванию функции потерь вдоль этой оси, появления оврагов, где схождение происходит долго.\n",
    "\n",
    "**Нормировка** - заключается в том, чтобы вычесть матожидание и поделить данные на стандартные отклонения по осям.\n",
    "- если стандартные отклонения одинаковые, то овраг останется, т.е. данные коррелируют\n",
    "- но если разделяющая поверхность проходит через математическое ожидание всей тренировочной выборки, то после нормализации она будет проходить через 0, и значит не нужно будет искать смещение\n",
    "\n",
    "$$\\vec {\\tilde x} = \\frac {\\vec x - \\vec \\mu}{\\vec \\sigma} \\\\\n",
    "\\vec \\mu = \\frac{\\sum \\vec {x_i}}{N} \\\\\n",
    "\\vec \\sigma = \\sqrt {\\frac {\\sum (\\vec x_i - \\vec \\mu)^2}{N-1}}$$\n",
    "\n",
    "Если мы обучаем по батчам, то это преобразование делается для каждого батча:\n",
    "\n",
    "$$\\vec {\\tilde x} = \\frac {\\vec x - \\vec \\mu_B}{\\vec \\sigma_B}\\cdot \\gamma + \\beta \\\\\n",
    "\\vec \\mu_B = \\frac{\\sum \\vec {x_i}}{N_B} \\\\\n",
    "\\vec \\sigma_B = \\sqrt {\\frac {\\sum (\\vec x_i - \\vec \\mu_B)^2}{N_B-1}}$$\n",
    "\n",
    "$\\gamma$ - обучаемый параметр, отвечает за СКО нейрона\n",
    "\n",
    "$\\beta$ - обучаемый параметр, отвечает за МО нейрона\n",
    "\n",
    "$\\gamma$, $\\beta$ - изменяются от батча к батчу, а статистика по МО и СКО данных накапливается в отдельных параметрах, которые и используются на этапе генерации ответов обученной сети:\n",
    "\n",
    "$$\\hat {\\vec \\mu} = EMA \\vec \\mu_B\\\\\n",
    "\\hat {\\vec \\sigma} = EMA \\sigma_B$$\n",
    "\n",
    "- скользящие экспоненциальные средние \n",
    "- с их помощью преобразуются (нормализуются) входные данные на тесте/валидации\n",
    "\n",
    "Почему мы не можем в процессе обучения заменить статистики, подсчитанные по батчу, на статистики, которые подсчитаны при помощи скользящих экспоненциальных средних? Здесь причина заключается вот в чём: когда мы рассматриваем статистики по батчу, то мы можем по каждой статистике сделать обратное распространение ошибки. Если бы она зависила от параметров, являющихся некоторой функцией предыдущих состояний, то нам надо было бы по ним по всем брать градиент. Смысл тогда батчевать выборку?\n",
    "\n",
    "**Очень эффективный прием обучения** - это очень легкая операция, которая практически не потребляет ни памяти, ни вычислительного времени.\n",
    "\n",
    "Куда можно добавить:\n",
    "\n",
    "- если свертка - вставляете батч-нормализацию двухмерную, \n",
    "- если полносвязанный слой - вставляйте одномерную батч-нормализацию.\n",
    "\n",
    "Куда нельзя добавить:\n",
    "\n",
    "- туда, где она не нужна\n",
    "  - уже центрированные данные\n",
    "  - сложные данные вроде текстовых эмбеддингов, т.е. где информация структурирована сложнее чем яркость пикселя в канале больше / меньше, например представляет собой вектор, который непонятно как нормализовывать не зная всех превсех вообще остальных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар: cлой нормализации\n",
    "\n",
    "Самая популярная версия слоя нормализации - слой нормализации \"по батчу\" (batch-norm слой).\n",
    "\n",
    "Рассмотрим его работу в наиболее простом случае, когда на вход подается батч из одномерных векторов:\n",
    "\n",
    "На вход подается батч одномерных векторов:\n",
    "\n",
    "$$x_i^{(j)}$$\n",
    "\n",
    "где $j$ индекс вектора внутри батча, $i$ - номер компоненты.\n",
    "\n",
    "Для текущего батча:\n",
    "\n",
    "- По каждой компоненте входа вычисляются мат.ожидание и дисперсия:\n",
    "\n",
    "$$E(x_i) = \\frac{\\sum_{j=1}^N x_i^{(j)}}{N} \\\\\n",
    "\\sigma (x_i)^2 = \\frac{\\sum_{j=1}^N (x_i^{(j)} - E(x_i))_j^2}{N}$$\n",
    "\n",
    "\n",
    "- Вход нормируется по формуле: \n",
    "\n",
    "$$z_i^{(j)} = \\frac{x_i^{(j)} - E(x_i)}{\\sqrt {\\sigma (x_i)^2 + \\varepsilon}}$$\n",
    "\n",
    "Эпсилон необходим для случая нулевой дисперсии.\n",
    "\n",
    "- Нормированный вход преобразуется следующим образом:\n",
    "\n",
    "$$y_i^{(j)} = z_i^{(j)} \\gamma_i + \\beta_i$$\n",
    "\n",
    "Где **Гамма** и **Бета** - обучаемые параметры слоя. Обратите внимание, Гамма и Бета - вектора такой же длины, как инстансы входа.\n",
    "\n",
    "Их можно фиксировать, например, простейший случай - Бета принимается равным нулевому вектору, Гамма - вектору из единиц. \n",
    "\n",
    "Если же взять Гамму равным знаменателю дроби из формулы для **Z**, а Бету равным мат.ожиданию, то слой вернет входной тензор без изменений. То есть, слой будет эквивалентен тождественной функции.\n",
    "\n",
    "Таким образом, параметры Бета и Гамма позволяют не терять входящию в слой информацию, и одновременно с этим, батч-норм слой нормализует вход. Последнее ускоряет сходимость параметров сети, а в некоторых случаях без нормализации добиться сходимости сети крайне сложно.\n",
    "\n",
    "Итоговая формула преобразования входа: \n",
    "\n",
    "$$y_i^{(j)} = \\frac{x_i^{(j)} - E(x_i)}{\\sqrt {\\sigma (x_i)^2 + \\varepsilon}} \\gamma_i + \\beta_i$$\n",
    "\n",
    "В этом уроке мы будем двигаться по следующему плану:\n",
    "\n",
    "- Вначале реализуем train-этап батч-нормализации для батча из одномерных векторов с нулевым Бета и единичным Гамма.\n",
    "- Затем добавим возможность задания параметров Бета и Гамма.\n",
    "- После этого добавим eval-этап использования слоя.\n",
    "- И последним шагом по батч-нормализации реализуем train-этап слоя батч-нормализации для батча из многоканальных двумерных тензоров с нулевым Бета и единичным Гамма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном шаге вам требуется реализовать функцию батч-нормализации без использования стандартной функции со следующими упрощениями:\n",
    "\n",
    "- Параметр Бета принимается равным 0.\n",
    "- Параметр Гамма принимается равным 1.\n",
    "- Функция должна корректно работать только на этапе обучения.\n",
    "- Вход имеет размерность число элементов в батче * длина каждого инстанса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def custom_batch_norm1d(input_tensor, eps):\n",
    "    # ex = input_tensor.mean(dim=0)\n",
    "    # sd = (((input_tensor - ex) ** 2).mean(dim=0) + eps).sqrt()\n",
    "    # normed_tensor = (input_tensor - ex) / sd\n",
    "\n",
    "    normed_tensor = ((input_tensor - input_tensor.mean(dim=0)) / \n",
    "                     torch.sqrt(input_tensor.var(dim=0, unbiased=False) + eps))\n",
    "    return normed_tensor\n",
    "\n",
    "input_tensor = torch.Tensor([[0.0, 0, 1, 0, 2], [0, 1, 1, 0, 10]])\n",
    "batch_norm = nn.BatchNorm1d(input_tensor.shape[1], affine=False)\n",
    "\n",
    "import numpy as np\n",
    "all_correct = True\n",
    "for eps_power in range(10):\n",
    "    eps = np.power(10., -eps_power)\n",
    "    batch_norm.eps = eps\n",
    "    batch_norm_out = batch_norm(input_tensor)\n",
    "    custom_batch_norm_out = custom_batch_norm1d(input_tensor, eps)\n",
    "\n",
    "    all_correct &= torch.allclose(batch_norm_out, custom_batch_norm_out)\n",
    "    all_correct &= batch_norm_out.shape == custom_batch_norm_out.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного обобщим функцию с предыдущего шага - добавим возможность задавать параметры Бета и Гамма.\n",
    "\n",
    "На данном шаге вам требуется реализовать функцию **батч-нормализации без использования стандартной функции** со следующими упрощениями:\n",
    "\n",
    "- Функция должна корректно работать только на этапе обучения.\n",
    "- Вход имеет размерность число элементов в батче * длина каждого инстанса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 7\n",
    "batch_size = 5\n",
    "input_tensor = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "def custom_batch_norm1d(input_tensor, weight, bias, eps):\n",
    "    normed_tensor = ((input_tensor - input_tensor.mean(dim=0)) / \n",
    "                     torch.sqrt(input_tensor.var(dim=0, unbiased=False) + eps))\n",
    "    return normed_tensor * weight + bias\n",
    "    \n",
    "\n",
    "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
    "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm_out = batch_norm(input_tensor)\n",
    "custom_batch_norm_out = custom_batch_norm1d(input_tensor, batch_norm.weight.data, batch_norm.bias.data, eps)\n",
    "print(torch.allclose(batch_norm_out, custom_batch_norm_out) \\\n",
    "      and batch_norm_out.shape == custom_batch_norm_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Избавимся еще от одного упрощения - реализуем работу слоя батч-нормализации на этапе предсказания.\n",
    "\n",
    "На этом этапе вместо статистик по батчу будем использовать экспоненциально сглаженные статистики из истории обучения слоя.\n",
    "\n",
    "В данном шаге вам требуется реализовать полноценный класс батч-нормализации без использования стандартной функции, принимающий на вход двумерный тензор. Осторожно, расчёт дисперсии ведётся по смещенной выборке, а расчет скользящего среднего по несмещенной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "input_size = 3\n",
    "batch_size = 5\n",
    "eps = 1e-1\n",
    "\n",
    "\n",
    "class CustomBatchNorm1d:\n",
    "    def __init__(self, weight, bias, eps, momentum):\n",
    "        self.weight = weight\n",
    "        self.bias =  bias \n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.ema_e =  0\n",
    "        self.ema_sd = 1\n",
    "        self.train = True\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        if self.train:\n",
    "            e_next = input_tensor.mean(dim=0)\n",
    "            sd_next = input_tensor.var(dim=0, unbiased=False)\n",
    "            ololo = input_tensor.shape[0] / (input_tensor.shape[0] - 1)\n",
    "            self.ema_e  = (1 - self.momentum)*e_next + self.momentum*self.ema_e\n",
    "            self.ema_sd = (1 - self.momentum)*sd_next*ololo  + self.momentum*self.ema_sd\n",
    "        else:\n",
    "            e_next, sd_next = self.ema_e, self.ema_sd \n",
    "\n",
    "        normed_tensor = (input_tensor - e_next) / torch.sqrt(sd_next + eps)\n",
    "        return normed_tensor * self.weight + self.bias\n",
    "\n",
    "    def eval(self):\n",
    "        self.train = False\n",
    "\n",
    "\n",
    "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
    "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.momentum = 0.5\n",
    "\n",
    "custom_batch_norm1d = CustomBatchNorm1d(batch_norm.weight.data,\n",
    "                                        batch_norm.bias.data, eps, batch_norm.momentum)\n",
    "\n",
    "all_correct = True\n",
    "\n",
    "for i in range(8):\n",
    "    torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "    norm_output = batch_norm(torch_input)\n",
    "    custom_output = custom_batch_norm1d(torch_input)\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, atol=1e-04) \\\n",
    "        and norm_output.shape == custom_output.shape\n",
    "\n",
    "batch_norm.eval()\n",
    "custom_batch_norm1d.eval()\n",
    "\n",
    "for i in range(8):\n",
    "    torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "    norm_output = batch_norm(torch_input)\n",
    "    custom_output = custom_batch_norm1d(torch_input)\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, atol=1e-04) \\\n",
    "        and norm_output.shape == custom_output.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слой батч-нормализации существует для входа любой размерности.\n",
    "\n",
    "В данном шаге рассмотрим его для входа из многоканальных двумерных тензоров, например, изображений.\n",
    "\n",
    "Если вытянуть каждый канал картинки в вектор, то вход будет трехмерным:\n",
    "\n",
    "- количество картинок в батче\n",
    "- число каналов в каждой картинке\n",
    "- число пикселей в картинке\n",
    "\n",
    "Процесс нормализации:\n",
    "\n",
    "- Вход разбивается на срезы, параллельные синей части. То есть, каждый срез - это все пиксели всех изображений по одному из каналов.\n",
    "- Для каждого среза считаются мат. ожидание и дисперсия.\n",
    "- Каждый срез нормализуется независимо.\n",
    " \n",
    "\n",
    "На данном шаге вам предлагается реализовать батч-норм слой для четырехмерного входа (например, батч из многоканальных двумерных картинок) без использования стандартной реализации со следующими упрощениями:\n",
    "\n",
    "- Параметр Бета = 0.\n",
    "- Параметр Гамма = 1.\n",
    "- Функция должна корректно работать только на этапе обучения.\n",
    "\n",
    "**По батчу**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "input_channels = 3\n",
    "batch_size = 3\n",
    "height = 10\n",
    "width = 10\n",
    "\n",
    "batch_norm_2d = nn.BatchNorm2d(input_channels, affine=False, eps=eps)\n",
    "\n",
    "input_tensor = torch.randn(batch_size, input_channels, height, width, dtype=torch.float)\n",
    "\n",
    "\n",
    "def custom_batch_norm2d(input_tensor, eps):\n",
    "    shape = input_tensor.shape\n",
    "    input_tensor = input_tensor.transpose(1, 0).flatten(1)  # измерение батча к кадру\n",
    "    normed_tensor = (input_tensor - input_tensor.mean(dim=1).unsqueeze(-1)) / \\\n",
    "                    torch.sqrt(input_tensor.var(dim=1, unbiased=False).unsqueeze(-1) + eps)\n",
    "    return normed_tensor.reshape(shape).transpose(1, 0)\n",
    "\n",
    "\n",
    "norm_output = batch_norm_2d(input_tensor)\n",
    "custom_output = custom_batch_norm2d(input_tensor, eps)\n",
    "print(torch.allclose(norm_output, custom_output) and norm_output.shape == custom_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы познакомились поближе с нормализацией \"по батчу\". Для упрощения дальнейшего изложения остановимся на случае трехмерного тензора на входе слоя, если же размерность входа больше трех, то вытянем все размерности кроме первых двух в одну размерность.\n",
    "\n",
    "Бывает нормировка не только по батчу, но и по другим измерениям.\n",
    "\n",
    "Обратите внимание на изображения ниже.\n",
    "\n",
    "<очень информативное изображение> \n",
    "\n",
    "Где:\n",
    "\n",
    "C - число каналов на входе.\n",
    "N - размер батча.\n",
    "H, W - размерность по последней (третьей) размерности входа.\n",
    "\n",
    "На изображении можно увидеть следующие виды нормализации:\n",
    "\n",
    "- По батчу.\n",
    "- По каналу.\n",
    "- По инстансу (эт типа вектор кадра в одном канале).\n",
    "- По группе (эт типа несколько векторов кадра в разных каналов).\n",
    "\n",
    "**По каналу**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-10\n",
    "\n",
    "def custom_layer_norm(input_tensor, eps):\n",
    "    \"\"\"по каналу, значит нам просто не нужно переставлять измерения, \n",
    "    канал и так рядом с кадром\"\"\"\n",
    "    shape = input_tensor.shape\n",
    "    input_tensor = input_tensor.flatten(1)\n",
    "    normed_tensor = (input_tensor - input_tensor.mean(dim=1).unsqueeze(-1)) / \\\n",
    "                    torch.sqrt(input_tensor.var(dim=1, unbiased=False).unsqueeze(-1) + eps)\n",
    "    return normed_tensor.reshape(shape)\n",
    "\n",
    "all_correct = True\n",
    "for dim_count in range(3, 9):\n",
    "    input_tensor = torch.randn(*list(range(3, dim_count + 2)), dtype=torch.float)\n",
    "    layer_norm = nn.LayerNorm(input_tensor.size()[1:], elementwise_affine=False, eps=eps)\n",
    "\n",
    "    norm_output = layer_norm(input_tensor)\n",
    "    custom_output = custom_layer_norm(input_tensor, eps)\n",
    "\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, 1e-2)\n",
    "    all_correct &= norm_output.shape == custom_output.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом шаге вам предлагается реализовать нормализацию \"по инстансу\" без использования стандартного слоя со следующими упрощениями:\n",
    "\n",
    "- Параметр Бета = 0.\n",
    "- Параметр Гамма = 1.\n",
    "- На вход подается трехмерный тензор (размер батча, число каналов, длина каждого канала инстанса).\n",
    "- Требуется реализация только этапа обучения.\n",
    "\n",
    "**По инстансу**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-3\n",
    "\n",
    "batch_size = 5\n",
    "input_channels = 2\n",
    "input_length = 30\n",
    "\n",
    "instance_norm = nn.InstanceNorm1d(input_channels, affine=False, eps=eps)\n",
    "\n",
    "input_tensor = torch.randn(batch_size, input_channels, input_length, dtype=torch.float)\n",
    "\n",
    "\n",
    "def custom_instance_norm1d(input_tensor, eps):\n",
    "    \"\"\"тут одномерим только кадр канала, среднее и СКО по последним измерениям\"\"\"\n",
    "    shape = input_tensor.shape\n",
    "    input_tensor = input_tensor.flatten(2)\n",
    "    normed_tensor = (input_tensor - input_tensor.mean(dim=2).unsqueeze(-1)) / \\\n",
    "                    torch.sqrt(input_tensor.var(dim=2, unbiased=False).unsqueeze(-1) + eps)\n",
    "    return normed_tensor.reshape(shape)\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "norm_output = instance_norm(input_tensor)\n",
    "custom_output = custom_instance_norm1d(input_tensor, eps)\n",
    "print(torch.allclose(norm_output, custom_output, atol=1e-06) and norm_output.shape == custom_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация \"по группе\" - это обобщение нормализации \"по каналу\" и \"по инстансу\".\n",
    "\n",
    "Каналы в изображении не являются полностью независимыми, поэтому возможность использования статистики соседних каналов является преимуществом нормализации \"по группе\" по сравнению с нормализацией \"по инстансу\".\n",
    "\n",
    "В то же время, каналы изображения могут сильно отличатся, поэтому нормализация \"по группе\" является более гибкой, чем нормализация \"по каналу\".\n",
    "\n",
    "На этом шаге вам предлагается реализовать нормализацию \"по группе\" без использования стандартного слоя со следующими упрощениями:\n",
    "\n",
    "- Параметр Бета = 0.\n",
    "- Параметр Гамма = 1.\n",
    "- Требуется реализация только этапа обучения.\n",
    "- На вход подается трехмерный тензор.\n",
    "\n",
    "**По группе**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "channel_count = 6\n",
    "eps = 1e-3\n",
    "batch_size = 6\n",
    "input_size = 2\n",
    "\n",
    "input_tensor = torch.randn(batch_size, channel_count, input_size)\n",
    "\n",
    "\n",
    "def custom_group_norm(input_tensor, groups, eps):\n",
    "    \"\"\"Expected number of channels in input to be divisible by num_groups\n",
    "    groups - это несколько соседних каналов в штуках\n",
    "    https://arxiv.org/pdf/1803.08494.pdf\"\"\"\n",
    "\n",
    "    N, C, input_size = input_tensor.shape\n",
    "    x = input_tensor.reshape(N, groups, C // groups * input_size)\n",
    "\n",
    "    mean = x.mean(dim=2).unsqueeze(-1)\n",
    "    var = x.var(dim=2, unbiased=False).unsqueeze(-1)\n",
    "    x = (x - mean) / torch.sqrt(var + eps)\n",
    "    x = x.reshape(N, C, input_size)\n",
    "    return x \n",
    "\n",
    "\n",
    "    shape = input_tensor.shape\n",
    "    input_tensor = input_tensor.flatten(2)\n",
    "    normed_tensor = (input_tensor - input_tensor.mean(dim=2).unsqueeze(-1)) / \\\n",
    "                    torch.sqrt(input_tensor.var(dim=2, unbiased=False).unsqueeze(-1) + eps)\n",
    "    return normed_tensor.reshape(shape)\n",
    "\n",
    "all_correct = True\n",
    "for groups in [1, 2, 3, 6]:\n",
    "    group_norm = nn.GroupNorm(groups, channel_count, eps=eps, affine=False)\n",
    "    norm_output = group_norm(input_tensor)\n",
    "    custom_output = custom_group_norm(input_tensor, groups, eps)\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, 1e-3)\n",
    "    all_correct &= norm_output.shape == custom_output.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические задачи: регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Снова рассмотрим задачу:**\n",
    "\n",
    "$$Xa = y$$\n",
    "\n",
    "где нужно найти вектор $\\vec a$ размера $N$, $X$ и $y$ - известны, $X$ - матрица размером $M \\times N$, $y$ - вектор размера $M$.\n",
    "\n",
    "Как мы уже видели, данную задачу можно представить в виде задачи оптимизации $L = \\|Xa - y\\| \\rightarrow min_{a}$.\n",
    "\n",
    "Как мы помним, как задача оптимизации, так и линейное уравнение, имеют бесконечное количество решений, если матрица $X^T$ имеет хотя бы одно нулевое собственное значение.\n",
    "\n",
    "С точки зрения оптимизационного процесса, все решения этой задачи оптимизации хороши, однако с точки зрения здравого смысла, слишком большие значения в векторе $\\vec a$ не нужны, если имеется решение с меньшими весами.\n",
    "\n",
    "Поэтому добавим регуляризацию в приведенную выше задачу оптимизации $L^* = \\|Xa - y\\|^2_2 + \\lambda \\|a\\|^2_2 \\rightarrow \\min_{a}$.\n",
    "\n",
    "**Запишите производную функции потерь** по вектору $\\vec a$, $\\frac {\\partial L^*}{\\partial a}$. Ответ представьте в матричном виде:\n",
    "\n",
    "$$\\frac{\\partial L^*}{\\partial a} = X^T \\cdot 2 (Xa - y) + \\lambda 2 a$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишите необходимое условие минимума для задачи оптимизации**\n",
    "\n",
    "$$L^* = \\|Xw - y\\|^2_2 + \\lambda \\|w\\|^2_2 \\rightarrow \\min_{w}$$\n",
    "\n",
    "**Сколько решений** имеет эта задача (при фиксированном $\\lambda > 0$)?\n",
    "\n",
    "- 1\n",
    "\n",
    "**Запишите решение задачи оптимизации в матричном виде**\n",
    "\n",
    "$$\\| Xa - y \\|_2^2 + \\lambda \\|w\\|^2_2 \\rightarrow \\min_{w} \\\\\n",
    "\\frac{\\partial L^*}{\\partial w} = X^T \\cdot 2 (Xw - y) + 2 \\lambda w= 0 \\\\\n",
    "... \\\\\n",
    "\\vec w = (X^T X + \\lambda E)^{-1}X^T \\vec y$$\n",
    "\n",
    "*) Такая регрессия называется гребневой регрессией (ridge regression). А гребнем является как раз диагональная матрица, которую мы прибавляем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу оптимизации:\n",
    "$$\\frac{1}{2}(wx - b)^2 + \\lambda \\left| w \\right| \\rightarrow \\min_{w}$$\n",
    "\n",
    "где $x$, $b$ - действительные числа, $\\lambda$ - неотрицательное число.\n",
    "\n",
    "Меньше какого значения $\\lambda$ данная задача имеет решение $w \\neq 0$?\n",
    "\n",
    "На пальцах примерно так\n",
    "$$\\to wx^2 - bx \\pm \\lambda = 0\\\\\n",
    "w = (bx \\pm \\lambda) / x^2 \\\\\n",
    "\\lambda < |bx|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Семинар**: Решаем задачу классификации на датасете CIFAR в `./Neural_Networks_and_CV`"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d00ba0b92f737b23b3e678e3a3ceb3fe4e948ad1ab95d9c6fdcbb4b4ec65f3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
