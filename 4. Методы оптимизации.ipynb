{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Содержание**<a id='toc0_'></a>    \n",
    "- [Самый обычный градиентный спуск](#toc1_)    \n",
    "- [Модификации градиентного спуска](#toc2_)    \n",
    "    - [Cтохастический градиентный спуск с импульсом](#toc2_1_1_)    \n",
    "    - [Cтохастический градиентный спуск с импульсом с заглядыванием вперед](#toc2_1_2_)    \n",
    "    - [Похожий вариант](#toc2_1_3_)    \n",
    "- [Некоторые другие методы оптимизации](#toc3_)    \n",
    "    - [Rprop](#toc3_1_1_)    \n",
    "    - [RMSprop](#toc3_1_2_)    \n",
    "    - [Adam](#toc3_1_3_)    \n",
    "- [Теоретические задачи: Понимаем SGD с momentum](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Самый обычный градиентный спуск](#toc0_)\n",
    "\n",
    "$$\\nabla f = [\\frac{\\partial f}{\\partial w_0}, ... ,\\frac{\\partial f}{\\partial w_n}]^T$$\n",
    "$$w^1 = w^0 - \\alpha \\nabla f(w^0) \\\\\n",
    "w^2 = w^1 - \\alpha \\nabla f(w^1) \\\\\n",
    "... \\\\\n",
    "w^{t+1} = w^t - \\alpha \\nabla f(w^t)$$\n",
    "\n",
    "\"Истинный\" градиент функции потерь должен вычисляться по потерям (средним, суммарным или еще как агрегированным) на всем объеме данных.\n",
    "\n",
    "Если вместо этого шаги делаются по градиенту потерь, расчитанному на **одном** примере, то такой метод называется \n",
    "**стохастическим градиентным спуском**.\n",
    "\n",
    "Если для расчета градиента функции потерь используется случайное подмножество исходных данных, то такой метод называется **мини-пакетным градиентным спуском** (по сути разновидность SGD, во многих пакетах для них используется одно название - SGD)\n",
    "\n",
    "Основное правило по выбору размера батча: \n",
    "- $ 1 \\ll |b| \\ll N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Модификации градиентного спуска](#toc0_)\n",
    "\n",
    "Основная проблема SGD - области \"овражного\" типа.\n",
    "\n",
    "### <a id='toc2_1_1_'></a>[Cтохастический градиентный спуск с импульсом](#toc0_)\n",
    "\n",
    "На помощь приходят аналогии из физики - катящийся шар (добавление инреции)\n",
    "\n",
    "$$ \\begin{cases} \n",
    "\\frac{\\partial v}{\\partial t} = \\frac{1}{m} (F + F_{тр}) = \\frac{1}{m} \\nabla f - \\frac{1}{m} \\eta v\\\\\n",
    "\\frac{\\partial x}{\\partial t} = v\n",
    "\\end{cases}$$\n",
    "\n",
    "- уравнения движения материальной точки (2-й з-н Ньютона: ускорение сила/массу, определение скорости)\n",
    "  - $F$ - антиградиент функции потерь (потенциальная сила)\n",
    "  - $F_{тр}$ - сила вязкого трения\n",
    "  - $\\eta$ - к-т вязкого трения\n",
    "\n",
    "Методом конечных разностей:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = \\lim_{\\Delta x \\to 0} \\frac{f(x - \\Delta x) - f(x)}{\\Delta x} \\approx \\frac{f(x - \\Delta x) - f(x)}{\\Delta x}$ - слева определение производной, справа - конечная разность, деленная на шаг\n",
    "\n",
    "\\*конечная разность при фиксированном шаге есть линейный оператор, отображающий пространство непрерывных функций в себя   \n",
    "\\*\\*разделённая разность — обобщение понятия производной для дискретного набора точек\n",
    "\n",
    "$$\\begin{cases} \n",
    "\\frac{\\partial v}{\\partial t} = \\frac{v^{t+1} - v^{t}}{\\Delta t}\\\\\n",
    "\\frac{\\partial x}{\\partial t} = \\frac{x^{t+1} - x^{t}}{\\Delta t}\n",
    "\\end{cases}$$\n",
    "\n",
    "Подставим это в исходную систему уравнений и выразим $v^{t+1}, x^{t+1}$\n",
    "\n",
    "$$\\begin{cases} \n",
    "v^{t+1}= v^t - \\frac{\\Delta t}{m} \\nabla f - \\frac{\\Delta t}{m} \\eta v^t \\\\\n",
    "x^{t+1} = x^t + v^t \\Delta t\n",
    "\\end{cases}$$\n",
    "\n",
    "С учетом, что координата это у нас веса, обозначив $\\alpha := \\Delta t $ (скорость обучения - это дискретность времени), $\\beta := 1 - \\frac{\\Delta t}{m} \\eta$, и приняв масштаб $\\frac{\\Delta t}{m} = 1$, получим:\n",
    "\n",
    "\n",
    "$$\\begin{cases} \n",
    "w^{t+1} = w^t + \\alpha v^t \\\\\n",
    "v^{t+1}= v^t \\beta - \\nabla f\n",
    "\\end{cases}$$\n",
    "\n",
    "Это называется **стохастический градиентный спуск с импульсом** (моментом). Параметры: скорость обучения $\\alpha$ и к-т импульса $\\beta$\n",
    "\n",
    "\\*) получается \"скорость\" - параметр, где градиент \"накапливается\" (когда двигаемся вниз) и \"расходуется\" (если пришлось двинуться вверх). Снижение \"скорости\" замедляет движение вдоль того направления, где мы \"влезаем на стену оврага\" и не трогает те направления, где мы движемся к минимуму. Ну логично, все как в жизни..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[Cтохастический градиентный спуск с импульсом с заглядыванием вперед](#toc0_)\n",
    "\n",
    "$$ \\begin{cases} \n",
    "\\frac{\\partial v}{\\partial t} = \\frac{1}{m} (F + F_{тр}) = \\frac{1}{m} \\nabla f - \\frac{1}{m} \\eta v\\\\\n",
    "\\frac{\\partial x}{\\partial t} = v\n",
    "\\end{cases}$$\n",
    "\n",
    "...\n",
    "\n",
    "$$\\begin{cases} \n",
    "w^{t+1} = w^t + \\alpha v^t \\\\\n",
    "v^{t+1}= v^t \\beta - \\nabla f (w^t + \\alpha v^t)\n",
    "\\end{cases}$$\n",
    "\n",
    "- что-то вроде дополнительной инерции, которая дополнительно уменьшает градиент если он пошел \"в гору\", но и дополнительно ускоряет, если идет вниз (?)... не понятно до конца"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[Похожий вариант](#toc0_)\n",
    "\n",
    "$$w^{t+1} = w^t - \\alpha EMA (\\nabla f)^t $$\n",
    "\n",
    "- EMA - экспоненциальное скользящее среднее градиента лосс-функции в момент времени t:\n",
    "\n",
    "$$EMA_{\\beta}(\\nabla f)^t = (1 -\\beta) \\nabla f^t + \\beta \\cdot EMA_{\\beta}(\\nabla f)^{t-1}$$\n",
    "\n",
    "- это такая \"функция с памятью\", она сглаживает последние несколько значений градиента (в данном примере – только текущее и предыдущее, логично инициализировать его нулём)\n",
    "\n",
    "Почти тоже, что обычный стохастический градиентный спуск с испульсом. Почему? В стохастическом градиентном спуске с испульсом у нас к скорости прибавлялись градиенты лосс-функции, а здесь у нас к координате прибавляются усреднённые градиенты лосс-функции, то есть раньше усреднение происходило в параметре скорости, а теперь усреднение происходит в экспоненциальном скользящем среднем. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Некоторые другие методы оптимизации](#toc0_)\n",
    "\n",
    "### <a id='toc3_1_1_'></a>[Rprop](#toc0_)\n",
    "\n",
    "В SGD и рассмотренных модификация одинаковые lr для всех параметров. Сделаем индивинуальный lr для каждого!\n",
    "\n",
    "Будем учитывать только знаки градиента: $sign (\\nabla f)$\n",
    "\n",
    "Возьмем начальный lr: $\\alpha_0$\n",
    "\n",
    "$$w_i^{t+1} = w_i^t - \\alpha_i^t \\cdot sign (\\nabla f_i(w^t))$$\n",
    "\n",
    "- по тем параметрам, по которым градиент растет, шаг будет сделан в противоположном направлении\n",
    "- сама величина тоже меняется\n",
    "$$\\alpha_i^{t+1} = \\begin{cases} \n",
    "\\alpha_i^t \\cdot 1.2 & sign (\\nabla f_i(w^t) \\cdot \\nabla f_i(w^{t-1}) ) > 0 \\\\\n",
    "\\alpha_i^t \\cdot 0.6 & sign (\\nabla f_i(w^t) \\cdot \\nabla f_i(w^{t-1}) ) \\leq 0 \n",
    "\\end{cases}$$\n",
    "\n",
    "- если знак градиента не менялся на последнем шаге, можно чуть ускорить спуск по этому параметру\n",
    "- если изменился или стоит на месте - сильно уменьшить шаг\n",
    "\n",
    "**Недостаток**: плохо работает с батчами, оптимизацию надо проводить для каждого элемента отдельно, при этом сильно эмпирические коэффициенты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[RMSprop](#toc0_)\n",
    "\n",
    "Root Mean Square Propagation. Сильно похож на Rprop\n",
    "\n",
    "$$w_i^{t+1} = w_i^t - \\alpha \\frac{\\nabla f(w^t)}{\\sqrt {EMA_{\\gamma}(\\nabla f^2)^t}}$$\n",
    "\n",
    "- Интуиция этого алгоритма заключается в том, что если вдоль какого-то направления производная слишком большая, это означает что вдоль этого направления нужно идти чуть-чуть помедленнее, потому что очень уж быстро изменяется функция потерь по этому направлению. А если вдоль какого-то другого параметра функция потерь слишком маленькая, то вдоль этого параметра нужно идти чуть побыстрее, потому что функция потерь вдоль этого параметра изменяется слишком медленно. Таким образом у нас подстраивается скорость обучения под каждый из параметров.\n",
    "- а средний шаг получится $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_3_'></a>[Adam](#toc0_)\n",
    "\n",
    "$$w_i^{t+1} = w_i^t - \\alpha \\frac{EMA_{\\beta_1}(\\nabla f)^t}{\\sqrt {EMA_{\\beta_2}(\\nabla f^2)^t} + \\varepsilon}$$\n",
    "\n",
    "- объединяет как положительные качества стохастического градиентного спуска с моментом (импульсом), и метода RMSProp\n",
    "- $\\varepsilon$ чтоб на 0 не поделить ненароком. Иногда его под корень заводят в заквадраченном виде, не суть\n",
    "- умолчания в торче и других пакетах $\\alpha= 3 \\cdot 10^{-4}$, параметр экспоненциального скользящего среднего в числителе - $\\beta_1=0.9$, $\\beta_2= 0.999$.\n",
    "  - Бывают ситуации, когда некоторые нейронные сети лучше учатся с какими-то другими параметрами алгоритма Adam. Однако для очень большого класса нейронных сетей вот такие параметры показывают очень хорошие результаты. \n",
    " \n",
    "Если вы хотите что-то быстро проверить и как-то обучить нейронную сеть, то вы можете взять алгоритм Adam с вот этими параметрами, и у вас получится какой-то хороший результат. \n",
    "\n",
    "Если же вы хотите сделать какую-то очень хорошо оптимизированную под текущую задачу нейронную сеть, то обычно берут стохастический градиентный спуск, и уже для стохастического градиентного спуска кропотливо подбирают необходимый learning rate, для того, чтобы стохастический градиентный спуск хорошо решал задачу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Теоретические задачи: Понимаем SGD с momentum](#toc0_)\n",
    "\n",
    "Материальная точка массы mm находится в поле потенциальных сил, которые заданы потенциалом $L(\\vec{x})$ (в momentum SGD в качестве потенциала берут лосс-функцию, которую нужно минимизировать). Потенциальная сила, которая действует на материальную точку, равна\n",
    "\n",
    "$$\\vec{F}_c = - \\nabla L(\\vec{x})$$\n",
    "\n",
    "Среда, в которой движется материальная точка, вязкая. Это означает, что, если материальная точка движется со скоростью $\\vec{v}$, то сила трения, которая действует на материальную точку со стороны среды, равна $\\vec{F}_f = - \\eta \\vec{v}$, где $\\eta$ - коэффициент вязкого трения. Принимая во внимание Второй Закон Ньютона:\n",
    "\n",
    "$\\vec{F} = m \\vec{a}$\n",
    "\n",
    "найдите ускорение материальной точки, если известна скорость $\\vec{v}$ и положение материальной точки $\\vec{x}$.\n",
    "\n",
    "$$\\vec a = \\frac {-\\nabla L(\\vec{x}) - \\eta  v}{m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принимая во внимание то, что скорость - это производная координат материальной точки по времени, а ускорение - это производная скорости по времени:\n",
    "$$\\vec{v} = \\frac{\\partial \\vec{x}}{\\partial t}$$\n",
    "$$\\vec{a} = \\frac{\\partial \\vec{v}}{\\partial t}$$\n",
    "\n",
    "перепишите выражение для ускорения из предыдущей задачи таким образом, чтобы в правой части остался 0 - это уравнение движения материальной точки. Перепишите его используя только координаты материальной точки (выразите ускорение и скорость через производные координаты) и их производные по времени.\n",
    "\n",
    "$$\\vec a = \\frac {-\\nabla L(\\vec{x}) - \\eta  \\frac{\\partial \\vec{x}}{\\partial t}}{m} =\\frac{\\partial \\vec{v}}{\\partial t} = \\frac{\\partial \\vec{x}}{\\partial t^2} \\\\\n",
    "\\frac{\\partial \\vec{x}}{\\partial t^2} + \\frac {1}{m} \\nabla L(\\vec{x}) + \\frac {1}{m} \\eta \\frac{\\partial \\vec{x}}{\\partial t} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как известно, энергия замкнутой системы остается постоянной. Допустим, что материальная точка начала движение с нулевой скоростью $\\vec{v}_0 = 0$, а потенциальная энергия в начальный момент времени была равна $L(x_0)$, а коэффициент трения $\\eta = 0$. В любой момент времени, согласно Закону Сохранения Энергии, выполняется равенство:\n",
    "$$\\frac{m v_t^2}{2} + L(x_t) = \\frac{m v_0^2}{2} + L(x_0)$$\n",
    "\n",
    "Какова будет потенциальная энергия материальной точки, когда в следующий раз она полностью остановится?\n",
    "\n",
    "$$E = L(x_0)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае, если в системе присутствует сила трения $\\eta$ (`eta`), энергия системы не сохраняется, а убывает. Запишите условие, наложенное на `eta` при котором рано или поздно материальная точка застрянет в некотором локальном минимуме потенциальной энергии $L$.\n",
    "\n",
    "$$\\eta > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дифференциальные уравнения можно решать численно. Для этого дифференциальные уравнения высоких порядков представляют в виде системы дифференциальных уравнений первого порядка. Наше уравнение можно представить следующим образом:\n",
    "\n",
    "$$\\begin{cases} \\frac{\\partial \\vec{x}}{\\partial t} = \\vec{v} \\\\ \\frac{\\partial \\vec{v}}{\\partial t} = - \\frac{1}{m}\\left( \\nabla L(\\vec{x}) + \\eta \\vec{v} \\right) \\end{cases}$$\n",
    " \n",
    "Теперь для решения такого уравнения применяются конечные разности. Приближением производной конечными разностями называется например такое приближение производной:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial t} = \\lim_{\\delta t \\rightarrow 0} \\frac{f_{t + \\delta t} - f_{t}}{\\delta t} \\approx \\frac{f_{t + \\Delta t} - f_{t}}{\\Delta t}$$\n",
    " \n",
    "при каком-то небольшом (но при этом конечном) шаге по времени $\\Delta t$. Стоит отметить, что конечные разности применяются не только для производных по времени, а могут применяться для любых производных. Также стоит отметить, что это не единственный способ приблизить производную функции при помощи конечных разностей.\n",
    "\n",
    "Допустим, в некоторый момент времени мы знаем координату и скорость материальной точки $\\vec{x}_t$, $\\vec{v}_t$. При помощи конечных разностей найдите выражение для скорости в следующий момент времени при условии, что правые части уравнений вычисляются из скоростей и координат в текущий момент времени.\n",
    "\n",
    "$$\\begin{cases} v^{t+1}= v^t - \\frac{\\Delta t}{m} \\nabla L(\\vec{x}) - \\frac{\\Delta t}{m} \\eta v^t \\\\\n",
    "x^{t+1} = x^t + v^t \\Delta t \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, что точка $x_0$, с которой мы начинаем оптимизацию, отстоит от точки оптимума на расстояние $\\rho$. Для оптимизации мы будем использовать алгоритм:\n",
    "\n",
    "$x_{t + 1} = x_t - \\alpha \\frac{\\nabla L}{\\sqrt{\\|\\nabla L\\|^2_2 + \\epsilon ^ 2}}$, где $\\epsilon \\ll \\|\\nabla L\\|_2$.\n",
    "\n",
    "Найдите минимальное число шагов, которое нужно будет сделать таким алгоритмом, чтобы достичь оптимума.\n",
    "\n",
    "$$steps = \\rho / \\alpha \\frac{\\nabla L}{\\sqrt{\\|\\nabla L\\|^2_2 + \\epsilon ^ 2}} \\approx \\rho / \\alpha$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взвешенное среднее некоторой величины ff оценивается следующим образом:\n",
    "$\\bar{f} = \\frac{\\sum_{i = 1}^N f_i w_i}{\\sum_{i = 1}^N w_i}$, где $w_i$ - веса, с которыми взвешиваются значения $f_i$.\n",
    "\n",
    "Допустим, что мы отслеживаем экспоненциальное скользящее среднее:\n",
    "\n",
    "$$EMA_\\alpha f_{t + 1} = \\alpha EMA_\\alpha f_{t} + (1 - \\alpha) f_{t + 1},$$\n",
    "\n",
    "причем в начальный момент времени $EMA_\\alpha f_0 = 0$.\n",
    "\n",
    "Если внимательно проанализировать эту процедуру, то можно понять, что экспоненциальное скользящее среднее не является взвешенным средним, а стремится к нему только при $t \\to \\infty$.\n",
    "\n",
    "Это особенно хорошо видно при $t = 1$. Допустим, $f_1 = 100$, $\\alpha = 0.9$. Тогда $EMA_1 f = 10$, что далеко от взвешенного среднего, которое для одного примера должно равняться $\\bar{f} = 100$ (поскольку наблюдался всего один пример и он был равен 100, каким бы вес ни был, взвешенное среднее должно быть равно 100).\n",
    "\n",
    "На какое значение должно быть разделено экспоненциальное среднее $EMA_\\alpha f_t$, чтобы получилось взвешенное среднее?\n",
    "\n",
    "После получения правильного ответа, подумайте, как нужно поправить алгоритм Adam:\n",
    "\n",
    "$$x_{t + 1} = x_t - \\alpha \\frac{EMA_{\\beta_1} \\nabla L_t}{\\sqrt{EMA_{\\beta_2} (\\nabla L) ^ 2 _t + \\epsilon}}$$\n",
    "чтобы в числителе и знаменателе были правильные оценки взвешенных средних на первых итерациях.\n",
    "\n",
    "**Если поделить на $1 - alpha^t$, то расписав рекуррентную формулу $EMA$ легко увидеть, что все \"забытые\" слагаемые будут \"вспомнены\" в той степени, в какой они к моменту $t$ \"забылись\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Семинар** про классификацию рукописных цифр в `./Neural_Networks_and_CV`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53d00ba0b92f737b23b3e678e3a3ceb3fe4e948ad1ab95d9c6fdcbb4b4ec65f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
