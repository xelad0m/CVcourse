{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Восстановление зависимости нейронной сетью\n",
    "\n",
    "Задача восстановления некоторой зависимости - регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборка делится обычно на 3 части:\n",
    "- train - обучающая, для подбора параметров нейросети\n",
    "- valid - для подбора гипер параметров обучения\n",
    "- test - для подтверждения результатов\n",
    "\n",
    "Пусть есть истинная зависимость $y = f(x)$ (например, скорости ветра от давления)\n",
    "\n",
    "- есть наблюдения $y_i = f(x_i) + \\epsilon_i$, где \n",
    "  - $y_i$ - наблюдаемая объясняемая (зависимая) переменная\n",
    "  - $x_i$ - наблюдаемый параметр\n",
    "  - $\\epsilon_i$ - ошибка наблюдения, которая может быть любой природы:\n",
    "    - влияние других неизвестных параметров\n",
    "    - некорректные наблюдения/измерения\n",
    "    - другие случайные и систематические ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Компоненты нейронной сети\n",
    "\n",
    "- архитектура нейронной сети\n",
    "- функция потерь\n",
    "- метод оптимизации\n",
    "- метрики (итогового качества решения задачи)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейронная сеть\n",
    "\n",
    "Пусть архитектура будет такая:\n",
    "- вход 1\n",
    "- выходной нейрон - линейный, т.е. выдает линейную комбинацию приходящих сигналов\n",
    "- между ними \"много\" сигмоидных нейронов с 1 выходом и выходом\n",
    "\n",
    "Нужно подобрать параметры выходного слоя (коэффициенты линейной комбинации) и скрытого слоя таким образом, чтобы выходной сигнал максимально близко аппроксимировал исходные данные $x_i, y_i$\n",
    "\n",
    "<img src=\"./img/approx.png\" width=\"700\">\n",
    "\n",
    "Т.е. целевая зависимость будет искаться в виде линейной комбинации отмасштабированных вдоль оси $x$ ($w_i x_i$) и сдвинутых на смещение ($b_i$) сигмоидных функций $\\sigma(w_i x_i + b_i)$.\n",
    "- первый нейрон должен хорошо приближать первую часть данных, второй - дальше, и т.д.\n",
    "\n",
    "Т.о. можно построить сколь угодно точную аппроксимацию любой непрерывной функции: \"универсальный аппроксиматор\"\n",
    "\n",
    "- **Теорема Цыбенко** (искусственная нейронная сеть прямой связи, в которых связи не образуют циклов, с одним скрытым слоем может аппроксимировать любую непрерывную функцию многих переменных с любой точностью)\n",
    "- **Теорема Колмогорова — Арнольда** (каждая многомерная непрерывная функция может быть представлена в виде суперпозиции непрерывных функций одной переменной) \n",
    "\n",
    "Более того, при помощи линейной комбинации сигмоид можно с любой точностью приблизить любую ограниченную функцию с не более, чем счетным числом разрывов.\n",
    "\n",
    "\\*\\* Существует масса других способов решать задачу регрессии. Например, это можно делать при помощи полиномов степени $N: P_N (x) = \\sum_{i = 0}^N a_i x^i$ и рядов Фурье (в частности, систем тригонометрических функций)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разделяющая поверхность в задаче регрессии** - \n",
    "\n",
    "- т.е. когда у нас активационная функция сигмоида - это область значений, где функция активации принимает значение 0,5 (порог классификации)\n",
    "\n",
    "Например, разделяющая поверхность для нейрона $y_1 = \\sigma(300 \\cdot x + 100)$ (почти ступенчатая функция), будет значительно левее по оси $x$, чем для нейрона $y_2 = \\sigma(2 \\cdot x - 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь\n",
    "\n",
    "Частый вариант - средний квадрат ошибки: $MSE = \\frac{1}{N} \\sum (\\hat y_i - y_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм настройки нейронной сети\n",
    "\n",
    "$W_0$ - исходное состояние сети (веса и смещения $b = w_0$)\n",
    "\n",
    "\n",
    "$\\nabla f = (\\frac{\\partial f}{\\partial w_0}, ... \\frac{\\partial f}{\\partial w_n})^T$ - вектор-градиент функции потерь\n",
    "\n",
    "$W_{k+1} = W_k - \\alpha \\nabla f(W_{k-1})$ - очередной шаг градиентного спуска, уточненное состояние сети, уменьшающее функцию потерь при определенных условиях\n",
    "\n",
    "Чтобы уменьшить функцию потерь мы должны знать насколько она изменяется при изменении любого параметра сети, т.е. мы должны знать все ее частные производные по всем параметрам.\n",
    "- именно поэтому реальные целевые функции задачи (точность классификации и т.п. не могут использоваться в качестве функций потерь)\n",
    "- нужны такие функции потерь (и такие функции активации). которые имеют производные, и чей минимум будет там же, где экстремум целевой функции задачи (метрики)\n",
    "  - дифференцируемые в большинстве точек\n",
    "    - итоговые целевые функции (метрики) не подходят (точность и т.п.)\n",
    "  - не равны 0 в большинстве точек\n",
    "    - пороговая функция активации не подходит\n",
    "\n",
    "Например:\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\hat y_i} = \\frac{2}{N} (\\hat y_i - y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Правило цепочки, производная сложной функции\n",
    "\n",
    "Чтобы посчитать производные по всем параметрам всех слоев сети нужно брать производные сложных функций, которые образуют **граф вычисления**, который определяется топологие сети. \n",
    "\n",
    "А в основе просто:\n",
    "\n",
    "$$\\frac{\\partial f(g(x), e(x))}{\\partial x} = \\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial x} + \\frac{\\partial f}{\\partial e} \\frac{\\partial e}{\\partial x}$$\n",
    "\n",
    "Еще раз - функция потерь сети это комбинация функций потерь и функций активации всех ее узлов, которые образуют граф вычисления. Обратное распространение ошибки - расчет производной функции потерь сети по всем, включая скрытые, параметрам, чтобы в ходе градиентного спуска изменять их все в направлении уменьшения функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические задачи: Графы вычислений и BackProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишите математическое выражение, которое задает этот граф\n",
    "\n",
    "<img src=\"./img/task_2_4_graph_01.png\" width=\"400\">\n",
    "\n",
    "y = c1 * (x1 + b1) * sigma(x2 + b2) + sigma(x1 + b1) * c2 * tanh(x2 + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чему будет равна производная $y$ по параметру $с_2$ в терминах частных производных промежуточных результатов\n",
    "\n",
    "d(y, c2) = d(y, z9) * d(z9, z6) * d(z6, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чему будет равна производная $y$ по параметру $b_1$\n",
    "\n",
    "d(y, b1) = d(y, z8) * d(z8, z7) * d(z7, z1) * d(z1, b1) + d(y, z9) * d(z9, z3) * d(z3, z1) * d(z1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чему будет равна производная $y$ по параметру $b_2$\n",
    "\n",
    "d(y, b2) = d(y, z8) * d(z8, z7) * d(z7, z4) * d(z4,z2) * d(z2, b2) + d(y, z9) * d(z9, z6) * d(z6, z5) * d(z5, z2) *d(z2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чему будет равна производная $\\tilde{y}$ по $с_2$, выраженная через входные данные и параметры\n",
    "\n",
    "d(y, c2) = tanh(x2 + b2) * sigma(x1 + b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чему будет равна производная $\\tilde{y}$ по $b_1$\n",
    "\n",
    "d(y, b1) = c1 * sigma(x2 + b2) + c2 * tanh(x2 + b2) * sigma(x1 + b1) * (1 - sigma(x1 + b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чему будет равна производная $\\tilde{y}$ по $b_2$\n",
    "\n",
    "d(y, b2) = c1 * (x1 + b1) * sigma(x2 + b2) * (1 - sigma(x2 + b2)) + sigma(x1 + b1) * c2 * (1 - tanh(x2 + b2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Рассмотрим линейное преобразование:**\n",
    "$$\\vec{y} = W \\vec{x} + \\vec{b}$$\n",
    " \n",
    "Допустим, что мы знаем производную \n",
    "$$\\frac{\\partial L}{\\partial \\vec{y}}$$\n",
    "\n",
    "Обозначим ее $\\vec{l}$.\n",
    "\n",
    "Найдите производную\n",
    "$$\\frac{ \\partial{L}} {\\partial \\vec{b}} = \\frac{\\partial L}{\\partial \\vec{y}} \\frac{ \\partial{y}} {\\partial \\vec{b}} = l$$\n",
    "\n",
    "Найдите производную\n",
    "$$\\frac{ \\partial{L}} {\\partial \\vec{x}} = \\frac{\\partial L}{\\partial \\vec{y}} \\frac{ \\partial{y}} {\\partial \\vec{x}} = W^T \\cdot l$$\n",
    "\n",
    "Найдите производную\n",
    "$$\\frac{ \\partial{L}} {\\partial W} = \\frac{\\partial L}{\\partial \\vec{y}} \\frac{ \\partial{y}} {\\partial \\vec{W}} = l \\cdot x^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические задачи: Восстановление зависимостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Мы рассматривали задачу** аппроксимации наблюдаемой одномерной зависимости $\\{ x_i, y_i \\}_{i=1}^N$ при помощи набора сигмоидных функций. $i$ здесь — номер наблюдаемого обьекта, $x_i$ — координата замера, $y_i$ — замеренное значение.\n",
    "\n",
    "Часто для решения этой задачи используется система степеней $\\tilde{y}_i = \\sum_{j=0}^M a_j x^j_i$, где $a_j$ — коэффициенты при степенях $x_i^j$, $\\tilde{y}_i$ — аппроксимированное значение функции. Это выражение можно записать в матричном виде:\n",
    "\n",
    "$\\tilde{y} = X a$, где\n",
    "\n",
    "$X = \\begin{bmatrix} x_0^0 & x_0^1 & x_0^2 & \\dots & x_0^M \\\\ x_1^0 & x_1^1 & x_1^2 & \\dots & x_1^M \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_N^0 & x_N^1 & x_N^2 & \\dots & x_N^M \\end{bmatrix} \\\\\n",
    "a = \\begin{bmatrix} a_0 & a_1 & a_2 & \\dots & a_M \\end{bmatrix}^T \\\\\n",
    "\\tilde{y} = \\begin{bmatrix} \\tilde{y}_0 & \\tilde{y}_1 & \\tilde{y}_2 & \\dots & \\tilde{y}_N \\end{bmatrix}^T$ \n",
    "\n",
    "Если мы хотим точно решить задачу, то мы должны найти решение системы линейных уравнений: $Xa = y$.\n",
    "\n",
    "Однако зачастую вместо непосредственного решения этого уравнения, которое сводится к вычислению обратной матрицы $X^{-1}$, решают задачу оптимизации (минимизации $L_2$-нормы):\n",
    "\n",
    "$\\| Xa - y \\|_2^2 \\rightarrow \\min_{a}$, где $\\| z \\|_2$ — длина вектора $z$.\n",
    "\n",
    "Докажите, что решение этой задачи оптимизации является и решением системы линейных уравнений $Xa = y$ в том случае, если эта система разрешима, вычислив производную от выражения $L(a) = \\|Xa - y\\|_2^2$ по всем параметрам $a$ и приравняв ее к нулю.\n",
    "\n",
    "**Док-во**:\n",
    "\n",
    "$$\\| Xa - y \\|_2^2 \\rightarrow \\min_{a} \\Leftrightarrow \\frac{\\partial L}{\\partial a} = X^T \\cdot 2 (Xa - y) = 0 \\Leftrightarrow Xa - y = 0 \\Leftrightarrow Xa = y \\Leftrightarrow a = X^{-1}y$$\n",
    "\n",
    "- т.е. если линейная система уравнений разрешима, то ее можно решать при помощи минимизации нормы разности левой и правой частей системы. Это эквивалентные задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **При какой степени** полинома $M$, полином $\\sum_{j=0}^M a_j x_i^j$ может точно пройти по всем точкам $\\{ x_i, y_i \\}_{i=0}^N$?\n",
    "\n",
    "- полином Лагранжа (многочлен минимальной степени, принимающий заданные значения в заданном наборе точек, то есть решающий задачу интерполяции): степень должна быть равна количеству точек (минус 1), но можно и больше, если че, т.е. $M >= N$\n",
    "- почему? потому что полином Лагранжа - это просто сумма базисных функций вида $\\prod_{j \\neq i} (x - x_i) / (x_i - x_j)$, которые равны 1 при $i=j$, иначе 0, умноженных на $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Рассмотрим** уравнение $y = X a$. При каком значении M существует единственное решение уравнения (и, соответственно, имеется единственное решение соответствующей оптимизационной задачи)?\n",
    "\n",
    "$M = N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Рассмотрим** уравнение $y = X a$. Как мы уже убедились, решением этого уравнения будет также и решение задачи оптимизации $L(a) = \\|Xa - y\\|_2^2$ в том случае, если система совместна (имеет решение).\n",
    "\n",
    "Однако, если матрица $X$ необратима (например, если она не квадратная), то, несмотря на то, что система уравнений может и не иметь решения, задача оптимизации в любом случае будет иметь решение. Найдите решение этой задачи оптимизации, исходя из условия $\\forall i \\ \\frac{\\partial L}{\\partial a_i} = 0$. Считайте, что $M \\leq N$\n",
    "\n",
    "Это будет означать, что точного решения нет, но есть проекция, которая будут максимально близка к точному решению (метод наименьших квадратов):\n",
    "\n",
    "$L(a) = (Xa-y)^{T}(Xa-y) \\rightarrow \\min _{a} \\Leftrightarrow X^T X a = X^T y \\Leftrightarrow a = (X^T X)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **В случае**, когда $M > N$, решение задачи оптимизации не единственно. Существует целое векторное пространство, добавление любого элемента из которого к любому решению задачи минимизации приводит к другому решению той же задачи.\n",
    "\n",
    "Найдите матрицу, собственные векторы которой, соответствующие ее нулевым собственным числам, образуют это векторное пространство (которое называется ядром или нулевым пространством матрицы $X$).\n",
    "\n",
    "$Xa = \\lambda a$ - собственные числа ($\\lambda$) и векторы ($a$) матрицы $X$ по определению. Такие векторы, которые остаются коллинеарными себе в результате преобразований линейного оператора $X$\n",
    "\n",
    "Но нас спрашивают про нулевые собственные числа, т.е. решения соответствующей однородной СЛАУ: $Xa=0$ \n",
    "\n",
    "И вообще, собственные векторы/числа имеют смысл только для квадратных матриц, поэтому $X^T X a = 0$, и ответ $X^T X$\n",
    "\n",
    "\\* Ортогональная матрица $Q: |Qx| = |x|$ (как оператор - не меняет длины векторов), а также $Q^T Q = E$. Это не в тему, просто"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.  **Как через все точки выборки** (всего N штук) может проходить бесконечное число полиномов степени M при M > N.\n",
    "\n",
    "Так и в нейронных сетях: если данных мало, а параметров слишком много, то возможна ситуация, когда не единственный набор параметров решает задачу.\n",
    "\n",
    "Это очень вредная ситуация, поскольку, если нейронная сеть очень хорошо приблизила тренировочные точки, cовсем не ясно, как она поведет себя на тестовых данных.\n",
    "\n",
    "Более того, поскольку решений оптимизационной задачи в такой ситуации бесконечно много, то на тестовых данных она может работать бесконечно плохо.\n",
    "\n",
    "Такая ситуация называется переобучение\n",
    "\n",
    "Какова размерность нулевого пространства (ядра) из предыдущей задачи? ($M > N$)\n",
    "\n",
    "**Ответ: $M-N$** (число )\n",
    "Пусть A: X→Y — линейный оператор. (В нашем случае A: M→N)  \n",
    "Ядром(нулевое пространство) линейного оператора A называется множество  KerA={x∈X∣Ax=0}  \n",
    "Образом линейного оператора A называется множество  ImA={y∈Y∣y=Ax} (множество значений)  \n",
    "\n",
    "Теорема (O ядре и базисе): dim(KerA)+dim(ImA)=n=dim(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Задача**\n",
    "\n",
    "Рассмотрим нейронную сеть, которая состоит из некоторого количества полносвязанных слоев с произвольной функцией активации.\n",
    "\n",
    "Рассмотрим некоторый слой этой нейронной сети, в котором имеется $M + 1$ нейронов. Он принимает на вход некоторый вектор $\\vec{x}$  размерности $N + 1$, который вычисляется предшествующими слоями, а возвращает вектор $\\vec{y}$ размерности $M$, который затем обрабатывается последующими слоями. Этот можно записать следующим образом:\n",
    "\n",
    "$\\vec{y} = f \\left(W^T \\vec{x} + \\vec{b} \\right),$\n",
    "\n",
    "где $f$ -- функция активации, матрица $W$ состоит из векторов весов для всех нейронов данного слоя: $W = \\left[ \\vec{w}_{0} , \\vec{w}_{1}, \\dots , \\vec{w}_{M} \\right]$, а вектор $\\vec{b} = \\left[ b_0, \\dots, b_M \\right]$ состоит из смещений для всех нейронов данного слоя.\n",
    "\n",
    "Если расширить вектор $\\vec{x}$ единицей (то есть, вместо вектора $\\vec{x} = \\left[ x_0, x_1, x_2, \\dots , x_N \\right]^T$, где $x_0, \\dots , x_t$ - координаты вектора, в качестве входа в первый слой рассматривать вектор $\\hat{\\vec{x}} = \\left[ 1, x_0, x_1, x_2, \\dots , x_N \\right]^T$, то первый слой нейронной сети можно переписать следующим образом:\n",
    "\n",
    "$\\vec{y} = f \\left(\\hat{W}^T \\hat{\\vec{x}}\\right),$\n",
    "\n",
    "где матрица $\\hat{W} = \\left[ \\hat{w}_{0}, \\hat{w}_{1}, \\dots , \\hat{w}_{M} \\right]$ состоит из векторов параметров для ВСЕХ параметров всех нейронов первого слоя, включая смещения и все веса: $\\hat{w}_{i} = \\left[ b_i, \\vec{w}_{i}^T \\right]^T$ - вектор параметров нейрона данного слоя с номером $i$.\n",
    "\n",
    "Если имеется несколько входных векторов в данный слой в обучающей выборке: $X = [\\vec{x}_0, \\vec{x}_1, \\dots, \\vec{x}_N]$, то мы можем вычислить результаты работы первого слоя нейронной сети сразу для всех векторов из обучающей выборки:\n",
    "\n",
    "$Y = f \\left( \\hat{W}^T \\hat{X} \\right).$\n",
    "\n",
    "**ЗАДАЧА**\n",
    "\n",
    "Найдите через $\\hat{W}, \\hat{X}$ такую матрицу, собственные векторы которой, соответствующие её нулевым собственным значениям, задают пространство, добавление любого элемента которого к вектору параметров любого нейрона данного слоя, не приводит к изменению работы данного слоя (и, следовательно, всей сети) ни на одном из векторов обучающей выборки.\n",
    "\n",
    "**РЕШЕНИЕ (херь какая-то но правильно вроде)**\n",
    "\n",
    "Не меняет, значит: $f(\\hat{W}^T \\hat{X}) = f ([\\hat{W}^T \\oplus C] \\hat{X}) \\rightarrow C: CX = 0 \\rightarrow C = X\\cdot X^T $ (почему?)\n",
    "\n",
    "Не меняет, значит производная по параметрам =0: $X = 0$. Но $X$ не квадратная, поэтому  $ X\\cdot X^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Допустим, у нас есть функция** $f(X) = \\sum{log_{e}(x_{ij} + 1)}$, где $X$ - тензор размера 2x2. Мы выбрали начальное приближение $X^{t=0} = [[1, 2], [4, 5]]$. И шаг градиентного спуска $\\alpha=10$.\n",
    "Чему будет равен $X^{t=1}$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [4., 5.]], requires_grad=True) <- X[t=0]\n",
      "tensor([[0.5000, 0.3333],\n",
      "        [0.2000, 0.1667]]) <- gradient\n",
      "tensor([[-4.0000, -1.3333],\n",
      "        [ 2.0000,  3.3333]], grad_fn=<SubBackward0>) <- X[t=1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "alpha = 10\n",
    "x = torch.tensor(\n",
    "    [[1.,  2.],\n",
    "     [4.,  5.]], requires_grad=True)\n",
    "\n",
    "\n",
    "function = (x+1).log().sum()\n",
    "function.backward()\n",
    "\n",
    "print (x, '<- X[t=0]')\n",
    "print(x.grad, '<- gradient')\n",
    "print(x - alpha * x.grad, '<- X[t=1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте расчет градиента для функции $f(w) = \\prod\\limits_{i,j}{log_{e}(log_{e}({w_{i,j} + 7}}))$ в точке $w = [[5, 10], [1, 2]]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0201, 0.0109],\n",
      "        [0.0449, 0.0351]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5463, grad_fn=<ProdBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w =  torch.tensor([[5, 10], [1, 2]]).float()\n",
    "w.requires_grad = True\n",
    "\n",
    "w =  torch.FloatTensor([[5,10],[1,2]]).requires_grad_(True) # как вариант\n",
    "\n",
    "function = torch.log(torch.log(w + 7)).prod()\n",
    "function = (w + 7).log().log().prod()   # ого, прикольно\n",
    "\n",
    "function.backward()\n",
    "\n",
    "print(w.grad)\n",
    "function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте градиентный спуск для той же функции $f(w) = \\prod\\limits_{i,j}{log_{e}(log_{e}({w_{ij} + 7}}))$\n",
    "\n",
    "Пусть начальным приближением будет $w^{t=0} = [[5, 10], [1, 2]]$, шаг градиентного спуска $alpha=0.001$.\n",
    "\n",
    "Чему будет равен $w^{t=500}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.9900, 9.9948],\n",
      "        [0.9775, 1.9825]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[5., 10.], [1., 2.]], requires_grad=True)\n",
    "alpha = 0.001\n",
    "\n",
    "for _ in range(500):\n",
    "    # critical: calculate the function inside the loop\n",
    "    function = (w + 7).log().log().prod()\n",
    "    function.backward()\n",
    "    w.data -= alpha * w.grad # put code here\n",
    "    w.grad.zero_()\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Семинар** по градиентному спуску в `./Neural_Networks_and_CV`"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d00ba0b92f737b23b3e678e3a3ceb3fe4e948ad1ab95d9c6fdcbb4b4ec65f3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
