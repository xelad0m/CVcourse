{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод максимального правдоподобия\n",
    "\n",
    "Независимые наблюдения\n",
    "$$\\{\\vec x_i\\}_{i=1}^N$$\n",
    "\n",
    "Вероятность получить именно такой набор наблюдений, с учетом их независимости\n",
    "\n",
    "$$P_w(\\{\\vec x_i\\}_{i=1}^N) = \\prod_{i=1}^N p_w(x_i) \\to \\max_w$$\n",
    "\n",
    "- $w$ - некоторый неизвестный параметр, от которого зависит распределение вероятностей сл.вел. $x$\n",
    "- хотим получить оценку этого параметра, при котором вероятность получить именно такой набор наблюдений максимальна\n",
    "\n",
    "$$w^* = \\argmax_w P_w(\\{\\vec x_i\\}_{i=1}^N)$$\n",
    "\n",
    "Переходим от произведения к сумме, переходя к логарифму (который не меняет точку минимума аргумента, т.к. это строго возрастающая функция: с мест говорят, что запись $\\log$ означает, что основание >1, первый раз слышу)\n",
    "\n",
    "$$\\argmax_w P_w(\\{\\vec x_i\\}_{i=1}^N) = \\argmax_w \\log P_w(\\{\\vec x_i\\}_{i=1}^N) \\\\\n",
    "\n",
    "\\log P_w(\\{\\vec x_i\\}_{i=1}^N) = \\log \\prod_{i=1}^N p_w(x_i)  = \\sum_{i=1}^N \\log p_w(x_i) \\to max_w$$\n",
    "\n",
    "- и все это ради того, что производая суммы это просто сумма производных, а производная произведения - та-дам, произведение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь - среднеквадратичная ошибка\n",
    "\n",
    "**Пусть $x \\sim N(\\mu, \\sigma)$** (нормальное распределние):\n",
    "\n",
    "$$ p_w(x_i) = \\frac{1}{\\sqrt {2 \\pi}\\sigma^2} \\exp(-\\frac{(x_i- \\mu)^2}{2 \\sigma^2}) \\\\\n",
    "w = \\{\\mu\\}$$\n",
    "\n",
    "Оценим $\\mu$:\n",
    "\n",
    "$$\\log p_w(x_i) = - \\log \\sqrt {2 \\pi} - \\log \\sigma - \\frac{(x_i- \\mu)^2}{2 \\sigma^2} \\\\\n",
    "\n",
    "\\sum_{i=1}^N \\log p_w(x_i) = - N \\log \\sqrt {2 \\pi} - N \\log \\sigma - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2  \\to \\max_{\\{\\mu\\}} \\\\\n",
    "\n",
    "\\argmax_w P_w = \\argmax_w \\log P_w = \\argmax_{\\{\\mu\\}} \\left[ - \\sum_{i=1}^N (x_i - \\mu)^2 \\right] \\\\\n",
    "\n",
    "\\argmax_w P_w = \\argmin_{\\{\\mu\\}} \\sum_{i=1}^N (x_i - \\mu)^2$$\n",
    "\n",
    "- получилось, чтобы оценить мат.ожидание сл.вел. надо найти минимум квадрата отклонения наблюдения от этой оценки \n",
    "- такая оценка даст максимально-правдоподобное значение матожидания сл.вел.\n",
    "- если нормировать на количество примеров, то получим функцию потерь в виде среднеквадратичной ошибки (MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь - бинарная кросс-энтропия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пусть $x \\sim B(N, p)$** (биномиальное распределение / схема Бернулли):\n",
    "- $N$ экспериментов, \n",
    "- $n$ - количество успехов,\n",
    "- $p$ - вер-ть успеха, \n",
    "\n",
    "$P(sequence) = p^n (1-p)^{N-n}$ - вероятность одного некоторого конкретного исхода, когда в $N$ испытаниях получили $n$ успехов\n",
    "\n",
    "$P(n) = C_N^n p^n (1-p)^{N-n}$ - вероятность получить любой из возможных исходов, когда в $N$ испытаниях получили $n$ успехов\n",
    "\n",
    "$C_N^n = \\frac{N!}{n!(N-n)!}$ - количество сочетаний из $N$ по $n$ (биномиальный коэфф.)\n",
    "\n",
    "$$\\argmax_p P(n) = \\argmax_p \\log P(n) = \\\\\n",
    "\n",
    "\\argmax_p \\left[ \\log C_N^n + n \\log p + (N-n) \\log(1-p) \\right] = \\\\\n",
    "\n",
    "\\argmax_p \\left[ \\frac{n}{N} \\log p + (1- \\frac{n}{N}) \\log(1-p) \\right] $$\n",
    "\n",
    "$$\\tilde p := \\frac{n}{N} \\to \\argmax_p \\left[ \\tilde p \\log p + (1- \\tilde p) \\log(1-p) \\right]$$\n",
    "\n",
    "- $\\tilde p$ - таргетное значение, полученное в результате наблюдений\n",
    "- $p$ - истинное значение, которое мы хотим оценить методом максимального правдоподобия\n",
    "\n",
    "$$\\partial f / \\partial p = \\frac{n}{N} \\frac{1}{p} - (1- \\frac{n}{N}) \\frac{1}{1-p} = 0$$\n",
    "$$p = \\frac{n}{N}$$\n",
    "\n",
    "- максимально-правдоподобная оценка вероятности в ГС - это частота в выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические задачи: метод максимального правдоподобия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим следующий процесс: мы замеряем некоторую величину $\\xi$, которая является количеством фотонов, которые регистрируются фоточувствительной пластиной. Пусть источник света в единицу времени генерирует $N$ фотонов. Каждый фотон с вероятностью $p_s$ может рассеяться на частицах среды, находящейся между источником и пластиной, и не достичь фоточувствительной пластины. С вероятностью $p_d$ фотон, достигший фоточувствительной пластины, может быть зарегистрирован пластиной. Найдите вероятность зарегистрировать в единицу времени $M$ фотонов.\n",
    "\n",
    "$A$ - (фотон НЕ рассеялся) И* (фотон зарегистировался): элементарные события независимые  \n",
    "$\\bar A$ - (фотон рассеялся) ИЛИ+ (фотон НЕ рассеялся И* фотон НЕ зарегистировался): а можно не расписывать элементарные события, а взять сразу $1 - P(A)$, результат будет тот же\n",
    "\n",
    "$$p(A) = (1−p_s​) p_d \\\\\n",
    "p(\\bar A) = p_s + (1−p_s)(1-p_d) = 1 - p_d + p_s p_d = 1- P(A)\\\\\n",
    "​p = C_N^M P(A)^M P(\\bar A)^{N-M} = C_N^M (p_d - p_s p_d )^M (1 - p_d + p_s p_d)^{N-M}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишите отрицательное логарифмическое правдоподобие (Negative Log Likelihood) для параметров $p_s$, $p_d$ из предыдущей задачи. Отбросьте все члены, которые не зависят от этих параметров.\n",
    "\n",
    "Negative Log Likelihood - меняет $\\argmax$ на $\\argmin$\n",
    "\n",
    "$$\\argmin_p \\left[ -\\frac{M}{N} \\log P(A) - (1- \\frac{M}{N}) \\log(1-P(A)) \\right] = \\\\\n",
    "\\argmin_p \\left[ -\\frac{M}{N} \\log ((1−p_s​) p_d) - (1- \\frac{M}{N}) \\log(1 - p_d + p_s p_d) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пусть мы наблюдали некоторую выборку** $m_1, \\dots m_k$. Мы знаем, что все эти измерения были порождены распределением Пуассона с параметром $\\lambda$:\n",
    "\n",
    "$$p(m) = \\frac{\\lambda^m}{m!} \\exp(-\\lambda)$$\n",
    "\n",
    "Запишите отрицательное логарифмическое правдоподобие для этого события относительно параметра $\\lambda$.\n",
    "\n",
    "$$\\argmax_{\\lambda} p_{\\lambda}(m) = \\argmin_{\\lambda} (-\\log p_{\\lambda}(m)) \\to\\\\\n",
    "\n",
    "\\argmin_{\\lambda} \\left[  \\sum_1^k -\\log \\frac{\\lambda^{m_k}}{m_k!} \\exp(-\\lambda) \\right] = \\\\\n",
    "\n",
    "\\argmin_{\\lambda} \\left[  - \\sum_1^k m_k \\log \\lambda - \\sum_1^k \\log m_k! + k \\lambda \\right] = \\\\\n",
    "\n",
    "\\argmin_{\\lambda} \\left[  - \\log \\lambda \\sum_1^k m_k + k \\lambda \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите параметр $\\lambda$ для предыдущей задачи, при котором достигается максимальное правдоподобие (соответственно, минимальное отрицательное логарифмическое правдоподобие).\n",
    "\n",
    "$$\\partial f / \\partial \\lambda = k - \\sum_1^k m_k / \\lambda = 0 \\\\\n",
    "\n",
    "\\lambda = \\frac{\\sum_1^k m_k}{k}$$\n",
    "\n",
    "- максимально правдоподобная оценка параметра распределения Пуассона по наблюдениям - это среднее значение наблюдений\n",
    "  - распределени Пуассона - количество событий произошедших за фиксированное время, при условии, что данные события происходят с некоторой фиксированной средней интенсивностью и независимо друг от друга\n",
    "  - параметр - средняя интенсивность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вернемся к нормальному распределению**. Пусть мы наблюдали некоторые значения $x_1, \\dots, x_N$, и мы знаем, что эти значения были порождены нормальным распределением:\n",
    "\n",
    "$$\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left( - \\frac{(x - \\mu)^2}{2 \\sigma^2} \\right)$$\n",
    "\n",
    "Найдите отрицательное логарифмическое правдоподобие для параметров $\\mu$ и $\\sigma$. Отбросьте все члены, в которые не входят $\\mu$ или $\\sigma$.\n",
    "\n",
    "$$\\log p_w(x_i) = - \\log \\sqrt {2 \\pi} - \\log \\sigma - \\frac{(x_i- \\mu)^2}{2 \\sigma^2} \\\\\n",
    "\n",
    "\\sum_{i=1}^N - \\log p_w(x_i) = N \\log \\sqrt {2 \\pi} + N \\log \\sigma + \\frac{1}{2 \\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2  \\to \\min_{\\{\\mu\\}} (\\to \\min_{\\{\\sigma\\}}) \\\\\n",
    "\n",
    "\\argmin_{\\{\\mu\\}} \\frac{1}{2 \\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2 \\\\\n",
    "\n",
    "\\argmin_{\\{\\sigma\\}} \\left[ N \\log \\sigma + \\frac{1}{2 \\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2 \\right]$$\n",
    "\n",
    "Найдите значения, при которых достигается максимальное правдоподобие.\n",
    "\n",
    "МО:\n",
    "\n",
    "$$\\partial f / \\partial \\mu = - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^N (x_i - \\mu)  = 0 \\\\\n",
    "\n",
    "\\mu = \\frac{\\sum_{i=1}^N x_i}{N}$$\n",
    "\n",
    "СКО: \n",
    "\n",
    "$$\\partial f / \\partial \\sigma = \\frac{N}{\\sigma}  - \\frac{1}{\\sigma^3} \\sum_{i=1}^N (x_i - \\mu)^2  = 0 \\\\\n",
    "\n",
    "\\frac{1}{\\sigma} (N - \\frac{1}{\\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2)  = 0 \\\\\n",
    "\n",
    "\\frac{1}{\\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2) = N \\\\\n",
    "\n",
    "\\sigma = \\sqrt {\\frac{\\sum_{i=1}^N (x_i - \\mu)^2}{N}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Формула Байеса**\n",
    "\n",
    "$$P(A|B) = P(B|A) P(A) / P(B)$$\n",
    "\n",
    "Допустим, что мы знаем $p(x|\\lambda)$ - вероятность наблюдать $x$ при условии параметра $\\lambda$. Допустим, что мы также откуда-то знаем вероятности различных значений $\\lambda: p(\\lambda)$.\n",
    "\n",
    "Найдите логарифм вероятности $p(\\lambda | x)$. Отбросьте все члены, которые не зависят от $\\lambda$.\n",
    "\n",
    "$$ \\log p(\\lambda | x) = \\log [ p(x | \\lambda) p(\\lambda) / p(x)] = \\\\\n",
    "\n",
    "\\log p(x | \\lambda) + \\log p(\\lambda) - \\log p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы наблюдали $x_0, ... x_N$, а также мы знаем, что\n",
    "\n",
    "$$p(x_i |\\mu) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( - \\frac{\\left(x_i - \\mu\\right)^2}{2} \\right)$$\n",
    "$$p(\\mu) = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left( - \\frac{\\mu^2}{2}\\right)$$\n",
    "\n",
    "Найдите отрицательный логарифм апостериорной вероятности $\\mu: \\log p(\\mu | x_i)$ и отбросьте все члены, которые не зависят от $\\mu$.\n",
    "\n",
    "Подумайте, как полученное выражение соотносится с регуляризацией.\n",
    "$$x = \\{x0, ... , x_N\\}$$\n",
    "$$ - \\log p(\\mu | x) = - \\log [ p(x | \\mu) p(\\mu) / p(x)] = \\\\\n",
    "\n",
    "\\log \\frac{1}{\\sqrt{2\\pi}} \\left[ \\sum \\frac{(x_i - \\mu)^2} {2} + \\frac{\\mu^2}{2}\\right] + \\log(p(x))$$\n",
    "\n",
    "- как это связано с регуляризацией\n",
    "\n",
    "Получили вывод \"метод Тихонова\", он же L2-регуляризация, он же метод максимального правдоподобия по Пуассону, где $\\frac{\\mu^2}{2}$ в выражении $\\sum \\frac{(x_i - \\mu)^2} {2} + \\frac{\\mu^2}{2}$ играет роль регуляризации, т.е. слагаемое, максимизирующее правдоподобие оценки мат.ожидания в ГС при условии получения такой выборки как $x$\n",
    "\n",
    "Это был **метод оценки с помощью апостериорного максимума (MAP)**, тесно связан с методом максимального правдоподобия (ML). Нужно только явно уточнить,что \n",
    "- $x \\sim N(\\mu,\\sigma_v)$\n",
    "- $\\mu \\sim N(0,\\sigma_m)$\n",
    "\n",
    "Таким образом, мы видим, что MAP оценка для $\\mu$ задана\n",
    "\n",
    "${\\displaystyle {\\hat {\\mu }}_{MAP}={\\frac {\\sigma _{m}^{2}}{n\\sigma _{m}^{2}+\\sigma _{v}^{2}}}\\sum _{j=1}^{n}x_{j}.}$\n",
    "\n",
    "\\*) MAP лежит в основе EM-алгоритма, который лежит в основе метода k-средних (который в таком виде превращаются в очень загадочную штуку, при своей элементарной и интуитивной геометрической интерпретациия)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d00ba0b92f737b23b3e678e3a3ceb3fe4e948ad1ab95d9c6fdcbb4b4ec65f3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
